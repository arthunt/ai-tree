{
  "metadata": {
    "title": "Дерево Знаний ИИ | AI Knowledge Tree",
    "description": "Комплексная интерактивная платформа для изучения и понимания концепций искусственного интеллекта."
  },
  "header": {
    "title": "Дерево Знаний ИИ",
    "description": "Освойте основы ИИ за ~2 часа",
    "treeView": "Карта Концепций",
    "treeViewAriaLabel": "Перейти к карте концепций",
    "classicView": "Учебный путь",
    "classicViewAriaLabel": "Вернуться к учебному пути"
  },
  "hero": {
    "title": "Дерево Знаний ИИ",
    "subtitle": "Поймите ИИ настолько, чтобы принимать осознанные решения. Освойте основы за ~2 часа.",
    "forWhom": "Для тренеров, руководителей команд и любознательных профессионалов",
    "classicViewTitle": "Учебный путь",
    "classicViewDescription": "Пошаговое освоение концепций",
    "currentPage": "Вы здесь",
    "treeViewTitle": "Карта Концепций",
    "treeViewDescription": "Визуальный обзор всех концепций",
    "clickHere": "Нажмите →",
    "or": "или",
    "treeEmoji": "Дерево",
    "bookEmoji": "Книга",
    "startFromRoots": "Начните от корней и двигайтесь вверх или листайте свободно",
    "newToAI": "Новичок в ИИ?",
    "beginnerPathDesc": "Начните здесь! Поймёте основы за ~30 минут.",
    "startWithTokens": "Начать с Токенов",
    "startLearning": "Начать обучение",
    "hook": "Вы уже используете ИИ. Теперь поймите, как он работает.",
    "showMore": "Показать ещё",
    "showLess": "Свернуть"
  },
  "landing": {
    "systemBadge": "Система Активна v2.1",
    "startEvolution": "НАЧАТЬ ЭВОЛЮЦИЮ",
    "beginJourney": "Начните путешествие с первого токена.",
    "evolutionaryPath": "Путь Эволюции",
    "chooseYourPath": "Выберите Свой Путь",
    "exploreQuestion": "Как вы хотите исследовать ИИ сегодня?",
    "theDna": "ДНК",
    "dnaTime": "Быстрый обзор (5 мин)",
    "dnaDesc": "Поймите основной механизм.",
    "recommendedStart": "Рекомендуемое начало",
    "theTree": "Дерево",
    "treeTime": "Полное исследование (15+ мин)",
    "treeDesc": "Глубокое погружение во все концепции.",
    "deepDive": "Погружение",
    "subhook": "Хватит гадать. Начните разбираться в биологических механизмах искусственного интеллекта.",
    "ctaPrimary": "Начать эволюцию",
    "ctaSecondary": "Карта концепций",
    "hook": "Вы используете ИИ каждый день. Теперь поймите, как он работает.",
    "orientation": "Бесплатный интерактивный курс. Регистрация не нужна."
  },
  "brand": {
    "heroOrigin": "От dendron (греч.: дерево) + дендриты (нейроны мозга, которые растут при обучении)",
    "footerTitle": "Почему Dendrix?",
    "footerStory": "Дендриты — это древовидные клетки мозга, которые отращивают новые ветви каждый раз, когда вы учитесь чему-то новому. Dendrix (dendron + дендриты) структурирует знания об ИИ так, как ваш мозг хранит информацию — от корней до листьев."
  },
  "levels": {
    "roots": "Корни",
    "trunk": "Ствол",
    "branches": "Ветви",
    "leaves": "Листья",
    "totalTime": "Полный путь: ~2 часа",
    "levelTime": {
      "roots": "~25 мин",
      "trunk": "~30 мин",
      "branches": "~35 мин",
      "leaves": "~30 мин",
      "fruits": "~20 мин",
      "orchard": "~15 мин"
    }
  },
  "treeView": {
    "title": "Дерево Знаний ИИ — Карта Концепций",
    "description": "Все концепции в одном виде",
    "instructionsTitle": "Интерактивная Карта Концепций",
    "instructionsText": "Наведите на концепцию, чтобы выделить её. Нажмите для подробностей.",
    "ariaLabel": "Карта концепций",
    "diagramAriaLabel": "Диаграмма концепций ИИ",
    "legendHeading": "Легенда уровней дерева",
    "rootsEmoji": "Росток",
    "trunkEmoji": "Вечнозелёное дерево",
    "branchesEmoji": "Растение",
    "leavesEmoji": "Листья",
    "rootsLabel": "1. Корни",
    "trunkLabel": "2. Ствол",
    "branchesLabel": "3. Ветви",
    "leavesLabel": "4. Листья",
    "fruitsLabel": "5. Плоды",
    "orchardLabel": "6. Сад",
    "introducedLabel": "Представлено",
    "keyPaperLabel": "Ключевая статья",
    "masterSkillLabel": "Освоить навык",
    "viewProgramLabel": "Смотреть программу",
    "conceptDeepDive": "Погружение в концепцию",
    "readMore": "Читать далее...",
    "exploreStage": "Исследовать этап {stage} →",
    "closeDetails": "Закрыть детали",
    "motif": "мотив",
    "noDescription": "Описание пока недоступно.",
    "loading": "Загрузка дерева...",
    "intentLabel": "Намерение: {intent}",
    "modeExploration": "Режим: Исследование",
    "lodSprout": "LOD 2: РОСТОК",
    "lodTree": "LOD 3: ДЕРЕВО"
  },
  "card": {
    "explore": "Изучить"
  },
  "viewMode": {
    "both": "Оба",
    "simple": "Простой",
    "technical": "Технический",
    "ariaLabel": "Выбор режима контента",
    "tooltip": {
      "simple": "Простые метафоры и повседневные примеры",
      "technical": "Технические объяснения для разработчиков",
      "both": "Показать оба варианта рядом"
    }
  },
  "nameToggle": {
    "simple": "Простые названия",
    "technical": "Технические названия",
    "ariaLabel": "Выбор отображения названий"
  },
  "darkMode": {
    "toggle": "Тёмный режим",
    "ariaLabel": "Выбор тёмного/светлого режима",
    "switchToDark": "Переключить на тёмный режим",
    "switchToLight": "Переключить на светлый режим"
  },
  "settings": {
    "ariaLabel": "Открыть настройки",
    "viewMode": "Режим просмотра",
    "theme": "Тема",
    "language": "Язык"
  },
  "welcome": {
    "title": "Добро пожаловать в Дерево Знаний ИИ",
    "subtitle": "Ваш гид по пониманию ИИ — за ~2 часа",
    "close": "Закрыть",
    "skip": "Пропустить",
    "back": "Назад",
    "next": "Далее",
    "getStarted": "Начать!",
    "stepOf": "Шаг {current} из {total}",
    "step1Title": "Метафора Дерева",
    "step1Desc": "Концепции ИИ, организованные как растущее дерево",
    "step1Item1": "Корни — Основы: Токены, Векторы, Внимание (начните здесь!)",
    "step1Item2": "Ствол — Инженерия: Промпты, Температура, Контекст",
    "step1Item3": "Ветви — Применение: RAG, Агенты, Вызов функций",
    "step1Item4": "Листья — Тренды: Green AI, Модели рассуждений, MoE",
    "step2Title": "Два способа исследования",
    "step2Desc": "Выберите подходящий формат",
    "step2Item1": "Учебный путь — Пошаговое освоение от корней до листьев",
    "step2Item2": "Карта концепций — Визуальный обзор всех концепций сразу",
    "step2Item3": "Поиск (Cmd+K) — Переход к любой концепции",
    "step3Title": "Отслеживайте прогресс",
    "step3Desc": "Учитесь в своём темпе",
    "step3Item1": "На каждую концепцию уходит 3-8 минут",
    "step3Item2": "Отмечайте концепции как изученные для отслеживания прогресса",
    "step3Item3": "Переключайтесь между Простым и Техническим объяснением в любой момент"
  },
  "footer": {
    "description": "Дерево Знаний ИИ — Интерактивный инструмент для изучения концепций ИИ",
    "version": "Версия",
    "reportIssue": "Сообщить о проблеме"
  },
  "complexity": {
    "1": "Основная идея",
    "2": "Взаимосвязанная",
    "3": "Глубокое погружение",
    "level": "Глубина концепции",
    "tooltip1": "Одна сфокусированная концепция — одна чёткая идея",
    "tooltip2": "Несколько связанных идей — понимание взаимосвязей",
    "tooltip3": "Плотная, многоуровневая тема с множеством предпосылок"
  },
  "loading": {
    "default": "Загрузка...",
    "concepts": "Загрузка концепций...",
    "tree": "Загрузка дерева...",
    "content": "Загрузка контента..."
  },
  "concept": {
    "simpleName": "Простое название",
    "metaphor": "Метафора",
    "closeAriaLabel": "Закрыть детальный вид",
    "closeDialog": "Закрыть диалог",
    "viewDetails": "Подробнее",
    "viewFullSize": "Полный размер",
    "beginnerPath": "Путь новичка",
    "readingTime": "мин чтения",
    "learnFirst": "Изучите сначала",
    "prerequisiteHelp": "Понимание этих тем поможет лучше усвоить концепцию:",
    "navigateTo": "Перейти к теме",
    "simpleMetaphor": "Простая метафора",
    "technicalExplanation": "Техническое объяснение",
    "codeExample": "Пример кода",
    "pressEscToClose": "Нажмите ESC или кликните снаружи, чтобы закрыть",
    "backToTree": "Назад к дереву",
    "share": "Поделиться концепцией",
    "shareNative": "Поделиться",
    "shareTwitter": "Поделиться в X (Twitter)",
    "shareLinkedIn": "Поделиться в LinkedIn",
    "copyLink": "Копировать ссылку",
    "linkCopied": "Ссылка скопирована!",
    "completed": "Завершено",
    "markAsComplete": "Отметить как понятое",
    "markedComplete": "Понятно!",
    "conceptMarkedComplete": "Концепция отмечена как изученная!",
    "undo": "Отменить",
    "simpleMode": "Простой",
    "technicalMode": "Технический",
    "explanation": "Объяснение",
    "visual": "Визуально",
    "code": "Код",
    "whyThisCode": "Зачем этот код?",
    "howToUse": "Как использовать",
    "tryItOut": "Попробуйте",
    "visualComingSoon": "Визуализация скоро будет",
    "visualComingSoonDesc": "Мы работаем над визуальным контентом для этой концепции.",
    "codeComingSoon": "Пример кода скоро будет",
    "codeComingSoonDesc": "Мы готовим практический пример кода для этой концепции.",
    "swipeHint": "Свайпните влево/вправо для предыдущей/следующей концепции",
    "copyCode": "Копировать код",
    "codeCopied": "Код скопирован!"
  },
  "tokenizer": {
    "title": "Демо Токенизатора",
    "subtitle": "Посмотрите, как ИИ видит текст",
    "showInfo": "Показать информацию",
    "hideInfo": "Скрыть информацию",
    "inputLabel": "Введите текст для токенизации",
    "inputPlaceholder": "Напишите что-нибудь, чтобы увидеть, как ИИ токенизирует текст...",
    "tokensLabel": "Токены:",
    "tokenCount": "{count} {count, plural, one {токен} few {токена} other {токенов}}",
    "tokensPlaceholder": "Токены появятся здесь...",
    "infoTitle": "Что такое токены?",
    "infoText": "Языковые модели видят текст не так, как мы. Они разбивают текст на маленькие части — <em>токены</em>. Токен может быть словом, частью слова или даже одним символом. Это демо показывает упрощённую версию!",
    "estimatedCost": "Примерная стоимость (GPT-4)",
    "costTip": "<strong>Совет:</strong> Длиннее текст = больше токенов = выше стоимость. Учитесь оптимизировать промпты!",
    "tryExamples": "Попробуйте примеры:",
    "example1": "Привет, мир!",
    "example2": "GPT-4 стоимость 2024",
    "example3": "Искусственный интеллект",
    "example4": "Машинное обучение — это увлекательно",
    "digitInsight": "<strong>Числа разбиваются на цифры!</strong> ИИ не видит «2024» как одно число — оно становится 4 отдельными токенами (2, 0, 2, 4). Каждая цифра получает свой <em>вектор</em> (числовое значение). Поэтому ИИ может ошибаться в математике — он обрабатывает цифры по одной, а не как целые числа.",
    "subwordInsight": "<strong>Длинные слова разбиваются!</strong> Токены с <code>##</code> — это части слов. ИИ разбивает незнакомые или длинные слова на более мелкие известные части, подобно тому, как вы произносите незнакомое слово по слогам.",
    "disclaimer": "Это упрощённый токенизатор. Реальные языковые модели используют более сложные методы, такие как BPE (Byte Pair Encoding) или WordPiece."
  },
  "sprout": {
    "phaseLabel": "Фаза 2: Основы",
    "title": "Росток",
    "subtitle": "Прежде чем дерево сможет отрастить ветви, ему нужны крепкие корни. Поймите 6 ключевых механизмов, на которых работают все модели ИИ.",
    "card": {
      "analogyLabel": "Аналогия",
      "readFull": "Читать полный урок",
      "tapToLearn": "Нажмите для обучения",
      "visualLink": "Нажмите для взаимодействия"
    },
    "inputPlaceholder": "Спросите об этих концепциях...",
    "hero": {
      "dormant": "СПЯЩЕЕ",
      "seed": "СЕМЯ",
      "dragGuide": "Перетащите узлы к семени",
      "context": "Контекст",
      "grammar": "Грамматика",
      "facts": "Факты",
      "feedbackContext": "Модель теперь понимает окружающие слова!",
      "feedbackGrammar": "Структура предложений формирует связи!",
      "feedbackFacts": "Знания о мире пускают корни!",
      "progress": "{connected} из {total} подключено",
      "complete": "Корни установлены! Росток готов расти.",
      "resetLabel": "Сбросить"
    }
  },
  "navigation": {
    "viewOptions": "Параметры просмотра",
    "levelSections": "Уровни концепций ИИ",
    "treeLevels": "Навигация по уровням дерева",
    "openNavPanel": "Открыть панель навигации",
    "closeNavPanel": "Закрыть панель навигации",
    "goToLevel": "Перейти к уровню {level}",
    "currentLevel": "Текущий",
    "conceptsCompleted": "концепций изучено",
    "backToTop": "Наверх",
    "switchToEstonian": "Переключить на эстонский",
    "switchToEnglish": "Переключить на английский",
    "levels": {
      "roots": "Корни",
      "trunk": "Ствол",
      "branches": "Ветви",
      "leaves": "Листья",
      "fruits": "Плоды",
      "sapling": "Питомник",
      "orchard": "Сад"
    }
  },
  "journey": {
    "title": "Ваш учебный путь",
    "subtitle": "От повседневного ИИ до глубокого понимания — как вырастить дерево от корней до листьев.",
    "roots": "Основы",
    "rootsDesc": "Токены, векторы и строительные блоки",
    "trunk": "Механизм",
    "trunkDesc": "Как модели обучаются и генерируют",
    "branches": "Применение",
    "branchesDesc": "RAG, агенты и реальные инструменты",
    "leaves": "Мастерство",
    "leavesDesc": "Файн-тюнинг, безопасность и не только"
  },
  "levelSection": {
    "conceptCount": "{count} концепций",
    "concepts": "Концепции уровня {level}"
  },
  "organicTree": {
    "clickForDetails": "Нажмите для подробностей",
    "simple": "Основная идея",
    "intermediate": "Взаимосвязанная",
    "advanced": "Глубокое погружение"
  },
  "search": {
    "button": "Поиск",
    "buttonLabel": "Открыть поиск (Cmd+K)",
    "placeholder": "Поиск концепций...",
    "inputLabel": "Поиск концепций ИИ",
    "instructions": "Используйте стрелки для навигации, Enter для выбора, Escape для закрытия",
    "close": "Закрыть поиск",
    "noResults": "Концепции не найдены",
    "tryDifferent": "Попробуйте другой запрос",
    "recentSearches": "Недавние поиски",
    "clearRecent": "Очистить",
    "popularConcepts": "Популярные концепции",
    "navigate": "Навигация",
    "select": "Выбрать",
    "resultsCount": "{count} {count, plural, one {результат} few {результата} other {результатов}}",
    "level": {
      "roots": "Корни",
      "trunk": "Ствол",
      "branches": "Ветви",
      "leaves": "Листья",
      "fruits": "Плоды",
      "orchard": "Сад"
    }
  },
  "skillSelector": {
    "title": "С чего начнём?",
    "subtitle": "Выберите ваш уровень",
    "close": "Закрыть окно",
    "beginner": {
      "title": "Новичок в ИИ",
      "description": "Начните с основ. Узнайте, что такое токены, векторы и трансформеры.",
      "time": "~30 мин для начала"
    },
    "intermediate": {
      "title": "Изучаю ИИ",
      "description": "Знаете основы? Погрузитесь в то, как модели учатся и генерируют контент.",
      "time": "~45 мин углублённо"
    },
    "advanced": {
      "title": "Строю с ИИ",
      "description": "Готовы к продвинутым темам: файн-тюнинг, RAG и агенты.",
      "time": "~35 мин продвинутых тем"
    },
    "skipAndExplore": "Пропустить и исследовать свободно"
  },
  "vectorDemo": {
    "title": "Демо Векторного Сходства",
    "subtitle": "Исследуйте, как ИИ понимает значения слов",
    "showInfo": "Показать информацию",
    "hideInfo": "Скрыть информацию",
    "reset": "Сброс",
    "word": "Слово",
    "optional": "(необязательно)",
    "wordPlaceholder": "напр. кот, счастье, король",
    "calculate": "Рассчитать сходство",
    "similarityScores": "Оценки сходства:",
    "similarityHigh": "Очень похожи",
    "similarityMedium": "Отчасти похожи",
    "similarityLow": "Не похожи",
    "visualization": "2D-визуализация:",
    "visualizationAriaLabel": "2D-визуализация сходства слов в векторном пространстве",
    "visualizationHint": "Слова ближе друг к другу — более схожи по значению. Наведите для подробностей.",
    "infoTitle": "Что такое эмбеддинги?",
    "infoText": "ИИ представляет слова как векторы (списки чисел), называемые <em>эмбеддингами</em>. Похожие слова имеют похожие векторы. Это демо использует упрощённые 10-мерные векторы, но реальные модели вроде GPT используют 1536+ измерений! 2D-график показывает эти многомерные отношения, спроецированные в видимое пространство.",
    "tryExamples": "Попробуйте примеры:",
    "exampleLabel1": "король, королева, принц",
    "exampleWords1": "king, queen, prince",
    "exampleLabel2": "кот, собака, машина",
    "exampleWords2": "cat, dog, car",
    "exampleLabel3": "счастливый, грустный, злой",
    "exampleWords3": "happy, sad, angry",
    "exampleLabel4": "яблоко, банан, пицца",
    "exampleWords4": "apple, banana, pizza",
    "exampleLabel5": "компьютер, телефон, дерево",
    "exampleWords5": "computer, phone, tree",
    "disclaimer": "Это демо использует заранее вычисленные упрощённые эмбеддинги. Реальные модели ИИ создают эмбеддинги динамически на основе контекста и используют тысячи измерений для более тонкого понимания."
  },
  "codeBlock": {
    "copied": "Скопировано!",
    "copiedAriaLabel": "Скопировано!",
    "copy": "Копировать",
    "copyAriaLabel": "Копировать код",
    "explanation": "Объяснение: "
  },
  "quickJump": {
    "ariaLabel": "Прокрутить вниз и начать обучение",
    "startLearning": "Начать обучение"
  },
  "skeleton": {
    "loadingTree": "Загрузка дерева..."
  },
  "toast": {
    "closeAriaLabel": "Закрыть уведомление"
  },
  "visuals": {
    "attention": {
      "title": "Механизм Внимания — Фокус",
      "inputSentence": "Входное предложение:",
      "word": {
        "went": "пошла",
        "store": "в магазин",
        "and": "и",
        "she": "она",
        "bought": "купила",
        "milk": "молоко"
      },
      "processing": "При обработке «она» (он/она/оно):",
      "strong": "Сильное (95%) — указывает на Мари",
      "medium": "Среднее (72%)",
      "explanationTitle": "Внимание показывает, какие слова важны для понимания каждого слова",
      "explanationLine1": "«она» уделяет сильное внимание «Мари», чтобы понять, о ком речь.",
      "explanationLine2": "Толщина стрелки = сила внимания. Это помогает модели понимать связи."
    },
    "prefillDecode": {
      "title": "Prefill vs Decode — Чтение vs Письмо",
      "prefillTitle": "PREFILL — Чтение",
      "decodeTitle": "DECODE — Письмо",
      "token": {
        "write": "Напиши",
        "me": "мне",
        "a": "одну",
        "story": "историю",
        "once": "Однажды",
        "upon": "давным"
      },
      "allAtOnce": "Всё обрабатывается сразу",
      "fastParallel": "Быстро и параллельно",
      "prefillSpeed": "~50мс на 100 токенов",
      "oneAtATime": "По одному токену за раз",
      "sequential": "Последовательно",
      "decodeSpeed": "~50мс на токен",
      "decodeExample": "(100 токенов = 5 секунд)",
      "whyMatters": "Почему это важно",
      "explanationLine1": "Prefill обрабатывает весь ваш запрос мгновенно (параллельно). Decode генерирует каждое новое слово по одному (последовательно).",
      "explanationLine2": "Вот почему длинные ответы занимают время, хотя чтение вашего вопроса — мгновенно."
    },
    "contextWindow": {
      "title": "Контекстное Окно — Рабочая Память",
      "tokenFlow": "Поток токенов:",
      "forgotten": "Забыто",
      "pastLimit": "За пределами",
      "activeContext": "АКТИВНЫЙ КОНТЕКСТ",
      "modelCanSee": "Модель может «видеть» и использовать это",
      "available": "Доступно",
      "spaceLeft": "Осталось места",
      "flowDirection": "Токены проходят через окно →",
      "sizesByModel": "Размеры контекстного окна по моделям",
      "gpt35Size": "4K токенов",
      "gpt4Size": "8K токенов",
      "gpt4TurboSize": "128K токенов",
      "claudeSize": "200K токенов",
      "visualScale": "Визуальный масштаб (1K = 1px ширины):",
      "explanation": "Представьте это как оперативную память: больше контекст = запоминает больше из разговора, но дороже и медленнее."
    },
    "hallucinations": {
      "title": "Галлюцинации — Уверенные Выдумки",
      "subtitle": "Оба ответа выглядят одинаково уверенно, но один полностью неверный!",
      "hallucination": "ГАЛЛЮЦИНАЦИЯ",
      "correct": "ВЕРНО",
      "question": "В: Кто построил Эйфелеву башню?",
      "answerStart": "О: Эйфелева башня была построена в",
      "wrongFact": "1887 году Клодом Моне",
      "correctFact": "1889 году Гюставом Эйфелем",
      "answerEnd": "в рамках Всемирной выставки в Париже.",
      "confidence94": "Уверенность: 94%",
      "confidence96": "Уверенность: 96%",
      "problem": "Проблема: Одинаковый уровень уверенности!",
      "whyTitle": "Почему возникают галлюцинации",
      "reason1": "• LLM генерируют текст на основе паттернов, а не фактов из базы данных",
      "reason2": "• Они не знают, когда выдумывают — просто предсказывают вероятные следующие слова",
      "reason3": "• Высокая уверенность ≠ точность. Модель может быть очень уверена в неверной информации",
      "tip": "Всегда проверяйте важные факты, особенно даты, имена и технические детали!"
    },
    "trainingInference": {
      "title": "Обучение vs Инференс — Школа vs Работа",
      "trainingTitle": "ОБУЧЕНИЕ",
      "trainingPhase": "(Фаза обучения)",
      "inferenceTitle": "ИНФЕРЕНС",
      "inferencePhase": "(Фаза использования)",
      "time": "Время",
      "cost": "Стоимость",
      "data": "Данные",
      "modelWeights": "Веса модели",
      "trainingTime": "Недели — месяцы",
      "trainingCost": "$2M — $100M+",
      "trainingGpus": "1000 — 10000",
      "trainingData": "Терабайты",
      "weightsChanging": "ИЗМЕНЯЮТСЯ",
      "trainingDesc": "Учится паттернам на миллиардах примеров",
      "inferenceTime": "Миллисекунды",
      "inferenceCost": "$0.002 за запрос",
      "inferenceGpus": "1 — 8",
      "inferenceData": "Ваш промпт",
      "weightsFrozen": "ЗАМОРОЖЕНЫ",
      "inferenceDesc": "Использует изученные паттерны для ответа на ваши вопросы",
      "explanation": "Обучение происходит один раз (дорого). Инференс — миллионы раз в день (дёшево и быстро)."
    },
    "transformers": {
      "title": "Трансформеры — Главная Архитектура",
      "subtitle": "Архитектура, стоящая за GPT, Claude, Llama и большинством современных моделей ИИ",
      "inputText": "ВХОДНОЙ ТЕКСТ",
      "tokenEmbedding": "Эмбеддинг Токена",
      "blocksLabel": "БЛОКИ ТРАНСФОРМЕРА × N (напр., 12-96 слоёв)",
      "multiHeadAttention": "Многоголовое Внимание",
      "feedForward": "Feed-Forward + Layer Norm",
      "repeatedN": "(повторяется N раз)",
      "outputProbs": "Выходные вероятности",
      "nextToken": "СЛЕДУЮЩИЙ ТОКЕН",
      "annotationAttention1": "← Где слова смотрят",
      "annotationAttention2": "друг на друга",
      "annotationFF1": "← Обработка и",
      "annotationFF2": "преобразование",
      "annotationOutput1": "← Предсказывает вероятности",
      "annotationOutput2": "следующего слова",
      "keyInnovation": "Ключевая инновация",
      "innovation1": "• Параллельная обработка",
      "innovation2": "• Механизм внимания",
      "innovation3": "• Масштабируется до миллиардов",
      "innovation3b": "параметров"
    },
    "temperature": {
      "title": "Температура: Регулятор Креативности",
      "lowLabel": "Низкая (0.0 — 0.3)",
      "lowDesc": "Предсказуемо",
      "medLabel": "Средняя (0.7)",
      "medDesc": "Сбалансировано",
      "highLabel": "Высокая (1.5 — 2.0)",
      "highDesc": "Хаотично",
      "probDistribution": "Распределение вероятностей",
      "exampleOutput": "Пример вывода",
      "lowExample1": "«Кот сидел на",
      "lowExample2": "коврике. Кот сидел на",
      "lowExample3": "коврике. Кот...»",
      "lowNote": "(повторяющееся, безопасное)",
      "medExample1": "«Кот отдыхал на",
      "medExample2": "тёплом коврике,",
      "medExample3": "наблюдая за птицами...»",
      "medNote": "(естественное, связное)",
      "highExample1": "«Квантовый велосипед",
      "highExample2": "мечтает! Фиолетовый",
      "highExample3": "слон Вторник...»",
      "highNote": "(случайное, бессвязное)"
    },
    "promptingBasics": {
      "title": "Как задавать хорошие вопросы",
      "badPrompt": "Плохой промпт",
      "badExample": "«Расскажи о еде»",
      "badResult1": "«Еда — это вещество, которое...»",
      "badResult2": "[расплывчатое эссе о еде]",
      "badResult3": "«...обеспечивает питание...»",
      "goodPrompt": "Хороший промпт",
      "goodExample1": "«Составь 3 вегетарианских рецепта",
      "goodExample2": "до 30 мин с 20+ г белка»",
      "goodResult1": "1. Нутовое карри (25 мин, 22 г)",
      "goodResult2": "2. Жареный тофу (20 мин, 24 г)",
      "goodResult3": "3. Чечевичный дал (28 мин, 21 г)",
      "principlesTitle": "5 Принципов промптинга",
      "principle1": "Будьте конкретны",
      "principle1Desc": "Укажите точно, что хотите",
      "principle2": "Дайте контекст",
      "principle2Desc": "Предоставьте нужную информацию",
      "principle3": "Задайте формат",
      "principle3Desc": "Укажите структуру вывода",
      "principle4": "Используйте примеры",
      "principle4Desc": "Покажите, что имеете в виду",
      "principle5": "Разбивайте задачи",
      "principle5Desc": "Делите на меньшие шаги"
    },
    "contextEngineering": {
      "title": "Контекстная Инженерия — Анатомия Промпта",
      "systemRole": "Системная Роль",
      "systemRoleExample": "«Ты — полезный ассистент по программированию...»",
      "rules": "Правила и ограничения",
      "rulesExample": "«Используй только Python. Макс 50 строк. Без внешних библиотек.»",
      "format": "Формат вывода",
      "formatExample": "«Верни JSON с ключами: answer, confidence, source»",
      "examples": "Примеры (Few-Shot)",
      "examplesExample": "«Вход: 'привет' → Выход: {greeting: true}»",
      "userQuery": "Запрос пользователя",
      "conclusion": "Лучший контекст = лучший вывод ИИ"
    },
    "ragPipeline": {
      "title": "RAG — Генерация с Извлечением",
      "query": "Запрос",
      "queryExample": "«Что такое...?»",
      "retrieve": "ИЗВЛЕЧЕНИЕ",
      "retrieveStep1": "1. Эмбеддинг запроса",
      "retrieveStep2": "2. Векторный поиск",
      "retrieveStep3": "3. Top-K результатов",
      "documentDb": "БД документов",
      "documentDbDesc": "Векторы + метаданные",
      "augment": "ДОПОЛНЕНИЕ",
      "augmentDesc1": "Соединить запрос +",
      "augmentDesc2": "найденный контекст",
      "generate": "ГЕНЕРАЦИЯ",
      "generateDesc1": "LLM создаёт",
      "generateDesc2": "обоснованный ответ",
      "answer": "Ответ",
      "answerLine1": "Обоснованный ответ с",
      "answerLine2": "указанием источников из",
      "answerLine3": "найденных документов"
    },
    "memoryTypes": {
      "title": "Типы Памяти ИИ",
      "shortTerm": "Краткосрочная Память",
      "shortTermDesc": "(Контекстное окно)",
      "message5": "Последнее сообщение пользователя",
      "message4": "Ответ ИИ",
      "message3": "Более раннее сообщение (угасает...)",
      "shortTermProp1": "Ограничена сессией",
      "shortTermProp2": "Фиксированный лимит токенов",
      "longTerm": "Долгосрочная Память",
      "longTermDesc": "(Внешнее хранилище)",
      "facts": "Факты о пользователе",
      "preferences": "Предпочтения",
      "history": "История разговоров",
      "patterns": "Изученные паттерны",
      "longTermProp1": "Сохраняется между сессиями",
      "longTermProp2": "Поиск через векторы",
      "store": "Сохранить →",
      "retrieveAction": "← Извлечь"
    },
    "lora": {
      "title": "LoRA — Адаптация Низкого Ранга",
      "baseModel": "Базовая Модель",
      "frozen": "Заморожена",
      "loraAdapters": "LoRA Адаптеры",
      "trained": "Обучаются",
      "layer": "Слой",
      "frozenWeights": "Замороженные веса",
      "whyLora": "Почему LoRA?",
      "benefit1": "Обучение 0.1% параметров",
      "benefit2": "В 100 раз дешевле полного",
      "benefit3": "Легко менять адаптеры",
      "benefit4": "Базовая модель нетронута",
      "input": "Вход ↓",
      "output": "Выход ↓"
    },
    "security": {
      "title": "Безопасность ИИ — Поверхности Атак",
      "inputZone": "ЗОНА ВХОДА",
      "modelZone": "ЗОНА МОДЕЛИ",
      "outputZone": "ЗОНА ВЫХОДА",
      "threat": "Угроза",
      "defense": "Защита",
      "inputThreat": "Атаки инъекции промптов",
      "inputDefense": "Валидация и фильтрация входа",
      "modelThreat": "Отравление обучающих данных",
      "modelDefense": "Выравнивание и RLHF",
      "outputThreat": "Генерация вредного контента",
      "outputDefense": "Фильтрация и ограждения выхода",
      "summaryTitle": "Глубокая защита: Защита на каждом уровне",
      "summaryDesc": "Одной защиты недостаточно — комбинируйте защиту входа, модели и выхода"
    },
    "agentLoop": {
      "title": "Цикл Агента ИИ",
      "observe": "НАБЛЮДАТЬ",
      "think": "ДУМАТЬ",
      "act": "ДЕЙСТВОВАТЬ",
      "toolSearch": "Поиск",
      "toolCode": "Код",
      "toolDatabase": "База данных",
      "consultant": "Консультант",
      "consultantDesc": "Даёт советы",
      "agent": "Агент",
      "agentDesc": "Выполняет действия"
    },
    "mcpArchitecture": {
      "title": "Архитектура MCP",
      "beforeMcp": "До MCP",
      "customIntegration": "Кастомная интеграция",
      "forEachTool": "для каждого инструмента",
      "afterMcp": "После MCP",
      "mcpServer": "MCP Сервер",
      "analogy": "Как универсальный адаптер для инструментов ИИ"
    },
    "complexityLevels": {
      "title": "Уровни Сложности ИИ",
      "llmChat": "LLM / Чат",
      "llmDesc": "Отвечает на вопросы",
      "reasoningModel": "Модель Рассуждений",
      "reasoningDesc": "Думает пошагово",
      "agentDoer": "Агент",
      "agentDesc": "Выполняет действия",
      "kitchenAnalogy": "Кухонная аналогия",
      "recipeBook": "Книга рецептов",
      "providesInfo": "Даёт информацию",
      "headChef": "Шеф-повар",
      "reasoning": "Рассуждение",
      "plansMenu": "Планирует меню",
      "cook": "Повар",
      "agentLabel": "Агент",
      "makesFood": "Готовит еду"
    },
    "agiAsi": {
      "title": "Спектр Возможностей ИИ",
      "narrowAi": "Узкий ИИ",
      "today": "(Сегодня)",
      "agi": "ОИИ",
      "future": "(Будущее?)",
      "agiDesc1": "Хорош во ВСЕХ",
      "agiDesc2": "человеческих задачах",
      "asi": "СИИ",
      "hypothetical": "(Гипотетический)",
      "asiDesc1": "Превосходит ВСЕ",
      "asiDesc2": "человеческие способности",
      "weAreHere": "Мы здесь"
    },
    "functionCalling": {
      "title": "Вызов Функций: Даём ИИ Руки",
      "user": "ПОЛЬЗОВАТЕЛЬ",
      "aiModel": "МОДЕЛЬ ИИ",
      "function": "ФУНКЦИЯ",
      "questionExample": "«Какая погода?»",
      "answerExample": "«5°C и дождливо»",
      "aiGenerates": "ИИ генерирует вызов",
      "appExecutes": "Ваше приложение выполняет"
    },
    "greenAi": {
      "title": "Green AI: Энергоэффективность",
      "trainingCost": "Стоимость обучения",
      "co2Emissions": "Выбросы CO₂",
      "inferenceCost": "Стоимость инференса (за запрос)",
      "reduction": "Снижение на 90%",
      "conclusion": "Такое же качество, доля стоимости"
    },
    "moe": {
      "inputQuery": "Входной запрос",
      "router": "Маршрутизатор",
      "expert": "Эксперт",
      "combinedOutput": "Объединённый выход",
      "activeExperts": "Активны: 2/8 экспертов",
      "fasterCheaper": "Быстрее + Дешевле"
    },
    "reasoningModels": {
      "title": "Обычная LLM vs Модель Рассуждений",
      "regularLlm": "Обычная LLM",
      "answer": "Ответ: 425",
      "sometimesWrong": "(иногда ошибается)",
      "fast": "Быстро (мгновенно)",
      "cheap": "Дёшево",
      "noVerification": "Без проверки",
      "reasoningModel": "Модель Рассуждений",
      "verifiedAnswer": "Ответ: 425 ✓",
      "slower": "Медленнее (многошаговая)",
      "verified": "Проверено и точно",
      "bestFor": "Лучше всего для: Математика, Код, Логика и Сложные рассуждения"
    }
  },
  "conceptData": {
    "moe": {
      "title": "Mixture of Experts (MoE)",
      "simpleName": "Совет экспертов",
      "explanation": "Модели MoE активируют только несколько специализированных «экспертов» для каждого запроса вместо использования всей модели. Как крупная компания, где разные отделы занимаются разными задачами. Это делает модель быстрее и дешевле при сохранении качества.",
      "metaphor": "Представьте большую больницу со множеством специалистов. Регистратор (маршрутизатор) читает ваши симптомы и направляет только к нужным врачам. Если проблема с кожей — к дерматологу. Если перелом — к ортопеду. Больница огромная, но вы посещаете только 2-3 специалистов за визит."
    },
    "agi-asi": {
      "title": "ОИИ и СИИ",
      "simpleName": "Сверхинтеллект",
      "explanation": "ОИИ (Общий Искусственный Интеллект) — это ИИ, способный сравниться с человеком во всех когнитивных задачах. СИИ (Сверхискусственный Интеллект) превосходил бы человеческий интеллект во всех областях. Сегодняшний ИИ «узкий» — отлично справляется с конкретными задачами, но лишён общего понимания.",
      "metaphor": "Сегодняшний ИИ — как блестящий студент, который отлично сдаёт один экзамен, но проваливает остальные (узкий ИИ). ОИИ — как Эйнштейн: мирового класса во всём. СИИ — интеллект настолько выше Эйнштейна, что мы не можем даже понять, что он понимает."
    },
    "green-ai": {
      "title": "Green AI (Устойчивость)",
      "simpleName": "Зелёный ИИ",
      "explanation": "Обучение крупных моделей ИИ потребляет огромное количество энергии. Green AI фокусируется на повышении эффективности: меньшие модели с аналогичной производительностью, оптимизированные методы обучения и эффективная аппаратная утилизация.",
      "metaphor": "Как переход от прожорливых внедорожников к электромобилям. Ранние модели ИИ были мощными, но расточительными. Green AI — это получение той же производительности от «Теслы» — оптимизированной, эффективной и устойчивой."
    },
    "reasoning-models": {
      "title": "Специализированные модели рассуждений",
      "simpleName": "Мыслитель",
      "explanation": "Модели рассуждений (o1, o3) не просто предсказывают следующее слово — они «думают» над проблемами пошагово. Они используют внутренние цепочки рассуждений, проверяя работу и исследуя подходы.",
      "metaphor": "Разница между блиц-шахматами и турнирными. Обычная LLM — как блиц-игрок: быстрый, интуитивный, но иногда ошибается. Модель рассуждений — как гроссмейстер, обдумывающий ход 30 минут."
    },
    "ai-agents": {
      "title": "Агенты ИИ",
      "simpleName": "Исполнитель",
      "explanation": "Агенты ИИ — автономные системы, которые могут использовать инструменты, принимать решения и выполнять многоэтапные задачи с минимальным вмешательством человека. Они работают в цикле: наблюдать → думать → действовать → наблюдать результат → продолжать.",
      "metaphor": "Разница между консультантом и работником. Обычный чат-бот — консультант: вы спрашиваете, получаете совет, но САМИ выполняете работу. Агент ИИ — работник: вы говорите «организуй поездку в Нью-Йорк на следующей неделе», и он бронирует рейсы, находит отели и создаёт события в календаре."
    },
    "mcp": {
      "title": "MCP (Model Context Protocol)",
      "simpleName": "Коннектор",
      "explanation": "MCP — открытый стандарт для безопасного подключения моделей ИИ к внешним источникам данных и инструментам через единый интерфейс. Как USB для ИИ: подключите один раз — работайте с любой совместимой моделью.",
      "metaphor": "Помните, когда у каждого телефона был свой кабель зарядки? Потом появился USB-C и решил проблему. MCP — та же идея для ИИ. Один коннектор, все устройства."
    },
    "complexity-levels": {
      "title": "3 Уровня Сложности",
      "simpleName": "Три уровня",
      "explanation": "Системы ИИ работают на трёх уровнях: (1) LLM — чистое предсказание текста; (2) Рассуждение — пошаговое решение задач; (3) Агент — автономное выполнение задач с инструментами, памятью и циклами решений.",
      "metaphor": "Представьте кухню: (1) Книга рецептов (LLM) — даёт инструкции. (2) Шеф (Рассуждение) — планирует меню с учётом ингредиентов. (3) Личный повар (Агент) — проверяет холодильник, заказывает продукты, готовит и убирает."
    },
    "function-calling": {
      "title": "Вызов Функций",
      "simpleName": "Даём ИИ Руки",
      "explanation": "Вызов функций позволяет LLM вызывать внешние функции для получения данных или выполнения действий. Модель решает, когда вызвать функцию, генерирует параметры, ваш код выполняет вызов, и модель использует результат.",
      "metaphor": "Представьте блестящего стажёра, который всё знает, но не имеет рук. Вызов функций — как дать ему телефон для звонков специалистам: «Позвони в метеослужбу», «Позвони в отдел базы данных»."
    },
    "context-engineering": {
      "title": "Контекстная инженерия",
      "simpleName": "Режиссура",
      "explanation": "Контекстная инженерия — тщательное конструирование полного окружения, в котором работает ИИ: роль, правила, формат, примеры и ограничения. Это радикально улучшает качество, последовательность и безопасность ответов.",
      "metaphor": "Как режиссура театрального актёра. Вы не просто даёте сценарий. Вы устанавливаете сцену, определяете персонажа, объясняете тон, показываете примеры и указываете желаемый результат."
    },
    "rag": {
      "title": "RAG (Генерация с Извлечением)",
      "simpleName": "Библиотека",
      "explanation": "RAG сочетает поиск информации с генерацией текста. Вместо того, чтобы полагаться только на обучающие данные модели, RAG сначала ищет в базе данных релевантные документы, затем включает их в промпт как контекст.",
      "metaphor": "Как экзамен с открытой книгой vs закрытой. Без RAG ИИ полагается только на память. С RAG ИИ может быстро найти релевантные страницы в библиотеке (вашей базе данных) и процитировать их."
    },
    "memory": {
      "title": "Память и Управление Состоянием",
      "simpleName": "Память",
      "explanation": "Память ИИ бывает двух типов: (1) Краткосрочная — контекстное окно разговора; (2) Долгосрочная — внешнее хранилище (базы данных), которое сохраняется между сессиями.",
      "metaphor": "Краткосрочная память — как блокнот на встрече: видны последние страницы, но старые заметки уходят. Долгосрочная память — ваш файловый шкаф: всё сохраняется навсегда и доступно для поиска."
    },
    "lora": {
      "title": "LoRA и Файн-тюнинг",
      "simpleName": "Повышение квалификации",
      "explanation": "LoRA (Low-Rank Adaptation) — техника эффективного дообучения больших моделей. Вместо переобучения всех миллиардов параметров, LoRA замораживает базовую модель и обучает маленькие «адаптерные» слои — всего 0.1% параметров.",
      "metaphor": "Представьте блестящего выпускника (базовая модель). Вместо 4 лет в университете (полный файн-тюнинг) для специализации в праве, вы даёте ему короткий 3-месячный курс (LoRA адаптер). Можно легко заменить юридический курс на медицинский."
    },
    "security": {
      "title": "Безопасность ИИ",
      "simpleName": "Безопасность",
      "explanation": "Безопасность ИИ включает защиту от трёх основных поверхностей атак: (1) Входные атаки — инъекция промптов; (2) Атаки на модель — отравление данных; (3) Выходные атаки — генерация вредного контента.",
      "metaphor": "Как охранник ночного клуба на трёх контрольных точках: (1) Вход — проверка документов; (2) Внутри — обученный персонал следит за порядком; (3) Выход — проверка, что никто не уносит ничего опасного."
    },
    "temperature-sampling": {
      "title": "Температура и Сэмплинг",
      "simpleName": "Регулятор Креативности",
      "explanation": "Температура контролирует случайность выводов ИИ, регулируя распределение вероятностей. Низкая температура (0.0-0.3) делает модель предсказуемой. Высокая (1.0-2.0) увеличивает креативность, но рискует бессвязностью.",
      "metaphor": "Температура — как регулятор на машине креативности. При 0 — калькулятор: всегда один ответ. При 0.7 — сбалансировано. При 2.0 — дикий художник, бросающий краску случайно."
    },
    "prompting-basics": {
      "title": "Основы Промптинга",
      "simpleName": "Искусство вопросов",
      "explanation": "Эффективный промптинг: (1) Будьте конкретны; (2) Дайте контекст; (3) Укажите формат; (4) Используйте примеры; (5) Разбивайте сложные задачи на более простые промпты.",
      "metaphor": "Как попросить совет по ресторанам. Плохо: «Расскажи о еде». Хорошо: «Предложи 3 вегетарианских ресторана в центре, до $30, открытых по воскресеньям, с террасой»."
    },
    "training-vs-inference": {
      "title": "Обучение vs Инференс",
      "simpleName": "Школа vs Работа",
      "explanation": "Обучение — дорогой одноразовый процесс на миллиардах примеров. Инференс — быстрое использование обученной модели за миллисекунды. Обучение стоит миллионы долларов, инференс — доли цента.",
      "metaphor": "Обучение — как медицинский факультет: годы учёбы, огромные инвестиции. Инференс — как врач на приёме: быстрые консультации на основе уже полученных знаний."
    },
    "tokens": {
      "title": "Токены",
      "simpleName": "Блоки текста",
      "explanation": "Токены — мельчайшие единицы текста, обрабатываемые моделями ИИ. Токен может быть словом, частью слова или символом. Токены определяют стоимость, скорость и лимиты контекста.",
      "metaphor": "Токены — как кубики LEGO для построения предложений. Частые слова — один кубик. Длинные или редкие слова разбиваются на несколько. ИИ видит не буквы, а эти кубики. Плата за API — за кубик, не за слово."
    },
    "vectors": {
      "title": "Векторы (Эмбеддинги)",
      "simpleName": "Карта Значений",
      "explanation": "Векторы представляют слова и концепции как списки чисел в многомерном пространстве. Похожие значения группируются вместе. Это позволяет ИИ выполнять семантическую математику: король - мужчина + женщина ≈ королева.",
      "metaphor": "Представьте, что каждое слово имеет GPS-координаты, но в 1536-мерном пространстве. Слова с похожими значениями — географические соседи. «Счастье» и «радость» — рядом. «Грусть» — далеко."
    },
    "attention": {
      "title": "Механизм Внимания",
      "simpleName": "Фокус",
      "explanation": "Внимание позволяет моделям динамически фокусироваться на релевантных частях входа при обработке каждого слова. Многоголовое внимание использует параллельные механизмы для захвата разных типов связей.",
      "metaphor": "Представьте чтение предложения с фонариком в тёмной комнате. Когда вы доходите до слова «она», вы направляете свет назад, чтобы найти, на кого ссылается «она». Яркость света — это вес внимания."
    },
    "prefill-decode": {
      "title": "Prefill vs Decode",
      "simpleName": "Чтение vs Письмо",
      "explanation": "Инференс LLM имеет две фазы: (1) Prefill — обработка всего запроса параллельно; (2) Decode — генерация ответа по одному токену последовательно. Вот почему чтение вопроса мгновенно, а длинный ответ — нет.",
      "metaphor": "Prefill — как прочитать всю страницу разом. Decode — как писать ответ от руки, буква за буквой. Чтение быстрое; написание требует времени."
    },
    "context-windows": {
      "title": "Контекстные Окна",
      "simpleName": "Рабочая Память",
      "explanation": "Контекстное окно — максимальный объём текста (в токенах), который модель может обработать за раз. При превышении лимита старые токены «забываются».",
      "metaphor": "Контекстное окно — как оперативная память. Модель с 4K окном — как 4ГБ RAM. Модель с 200K — как 64ГБ. При нехватке памяти система «забывает» старые данные."
    },
    "hallucinations": {
      "title": "Галлюцинации",
      "simpleName": "Уверенные выдумки",
      "explanation": "Галлюцинации — когда ИИ генерирует правдоподобную, но фактически неверную информацию. Модели предсказывают текст по паттернам, а не по истине. Опасность: галлюцинации подаются с такой же уверенностью, как факты.",
      "metaphor": "Представьте студента, который не знает ответа, но уверенно выдумывает правдоподобный. «Кто изобрёл телефон?» — «Томас Эдисон, в 1875, в Бостоне.» Звучит правдоподобно, но неверно. Всегда проверяйте важные утверждения."
    },
    "transformers": {
      "title": "Трансформеры",
      "simpleName": "Главная Архитектура",
      "explanation": "Трансформеры — архитектура нейронных сетей, стоящая за практически всеми современными LLM. Они произвели революцию, обеспечив параллельную обработку и захват дальних зависимостей через механизмы внимания.",
      "metaphor": "Трансформеры — как стальной каркас, позволивший строить небоскрёбы. До них здания (модели) были ограничены 10 этажами. Стальные каркасы (трансформеры) позволили строить 100+ этажей, складывая модульные блоки."
    }
  },
  "conceptLevels": {
    "roots": {
      "name": "КОРНИ",
      "subtitle": "Фундаментальная Механика",
      "description": "Это «двигатель» ИИ. Без понимания того, как машина обрабатывает язык, вы действуете вслепую."
    },
    "trunk": {
      "name": "СТВОЛ",
      "subtitle": "Инженерия и Архитектура",
      "description": "Ствол — опорная структура, которая держит всё остальное."
    },
    "branches": {
      "name": "ВЕТВИ",
      "subtitle": "Применения и Агенты",
      "description": "Ветви — практическое применение знаний."
    },
    "leaves": {
      "name": "ЛИСТЬЯ И ПЛОДЫ",
      "subtitle": "Исследования и Тренды",
      "description": "Самая быстро меняющаяся часть дерева. То, что передовое сегодня, станет стандартом завтра."
    }
  },
  "codeSnippets": {
    "tokens": {
      "sampleText": "Привет, как дела?",
      "comment_load": "Загрузка токенизатора для GPT-4",
      "comment_tokenize": "Токенизация текста",
      "comment_decode": "Декодирование обратно в текст"
    },
    "vectors": {
      "word1": "king",
      "word2": "queen",
      "word3": "banana",
      "comment_create": "Создание эмбеддингов",
      "comment_similarity": "Расчёт сходства (косинусное сходство)",
      "result1": "Король vs Королева",
      "result2": "Король vs Банан"
    },
    "attention": {
      "comment_simplified": "Упрощённый расчёт внимания",
      "comment_current": "Текущее слово",
      "comment_previous": "Предыдущие слова",
      "comment_simplified2": "Упрощённо",
      "comment_scores": "Расчёт оценок внимания",
      "word1": "Кот",
      "word2": "сидел",
      "word3": "стул",
      "comment_apply": "Применение внимания"
    },
    "ai-agents": {
      "comment_tools": "Определение доступных инструментов",
      "comment_loop": "Цикл агента",
      "userMessage": "Какая погода в Лондоне?",
      "toolResult_temp": "5°C",
      "toolResult_cond": "облачно",
      "comment_execute": "Выполнение инструмента",
      "comment_final": "Финальный ответ"
    },
    "context-engineering": {
      "comment_poor": "Плохой контекст — размытый промпт",
      "poorPrompt": "Напиши письмо",
      "systemPrompt": "Ты профессиональный специалист по деловой переписке.\n      \n      Правила:\n      - Используй деловой тон\n      - Письма короткие (макс 150 слов)\n      - Всегда включай призыв к действию\n      - Избегай жаргона\n      \n      Формат:\n      Приветствие -> Контекст -> Основное сообщение -> Призыв к действию -> Закрытие",
      "userPrompt": "Напиши письмо клиенту с вопросом о сроках доставки продукта"
    },
    "rag": {
      "comment_kb": "База знаний",
      "doc1": "Столица Эстонии — Таллин.",
      "doc2": "Таллин расположен на берегу Финского залива.",
      "doc3": "Python — популярный язык программирования.",
      "comment_query": "Запрос пользователя",
      "query": "Какая столица Эстонии?",
      "comment_retrieve": "1. Извлечение: Поиск релевантных документов",
      "comment_augment": "2. Дополнение: Добавление контекста в промпт",
      "promptTemplate": "Контекст: {top_doc}\\n\\nВопрос: {query}",
      "comment_generate": "3. Генерация: Получение ответа"
    },
    "function-calling": {
      "comment_define": "Определение доступных функций",
      "toolDescription": "Получить прогноз погоды для города",
      "paramDescription": "Название города",
      "comment_user": "Пользователь спрашивает о погоде",
      "userMessage": "Какая будет погода завтра в Лондоне?",
      "comment_decides": "Модель решает вызвать функцию",
      "comment_execute": "Выполнение функции (имитация)",
      "toolResult_temp": "8°C",
      "toolResult_cond": "дождливо",
      "comment_sendBack": "Отправка результата обратно в модель",
      "comment_getFinal": "Получение финального ответа"
    }
  },
  "codeExplanations": {
    "tokens": "Этот пример показывает, как библиотека tiktoken конвертирует текст в токены. Русские слова могут требовать больше токенов, чем английские.",
    "vectors": "Векторы конвертируют слова в числа. Слова с похожими значениями (king/queen) ближе в пространстве, чем разные (king/banana).",
    "attention": "Механизм внимания рассчитывает важность каждого слова относительно других. Более высокие оценки указывают на более сильные связи.",
    "context-engineering": "Контекстная инженерия — создание системного окружения: роль, правила, формат и цель. Это значительно улучшает точность и полезность ответов ИИ.",
    "rag": "RAG находит релевантные документы (Retrieve), добавляет их в промпт (Augment) и генерирует ответ (Generate). Это позволяет ИИ отвечать точно на основе данных компании.",
    "ai-agents": "Агенты ИИ используют цикл: думать → выбрать инструмент → выполнить → проверить результат → повторить. Это позволяет автономно выполнять задачи.",
    "function-calling": "Вызов функций позволяет ИИ «решать», когда использовать внешние инструменты. Модель генерирует структурированный вызов, приложение выполняет его, и модель использует результат для финального ответа."
  },
  "learningPaths": {
    "title": "Учебные Пути",
    "subtitle": "Курируемые маршруты по концепциям ИИ. Выберите путь, соответствующий вашим целям.",
    "backToHome": "На главную",
    "backToPaths": "Все учебные пути",
    "concepts": "концепций",
    "minutes": "мин",
    "difficulty": "Сложность",
    "beginner": "Начинающий",
    "intermediate": "Средний",
    "advanced": "Продвинутый",
    "estimatedTime": "Примерное время",
    "prerequisites": "Предпосылки",
    "noPrerequisites": "Без предпосылок",
    "startPath": "Начать обучение",
    "continuePath": "Продолжить обучение",
    "completed": "Завершено",
    "progress": "Прогресс",
    "conceptsCompleted": "{count} из {total} завершено",
    "nextConcept": "Далее",
    "pathComplete": "Путь завершён!",
    "pathCompleteDesc": "Вы прошли все концепции этого учебного пути.",
    "exploreMore": "Другие пути",
    "openConcept": "Открыть концепцию",
    "ai-fundamentals": {
      "title": "Основы ИИ",
      "description": "Постройте прочный фундамент. Поймите токены, векторы, внимание и как работают трансформеры.",
      "longDescription": "Этот путь проведёт вас от нуля до свободного владения основными механизмами современного ИИ. Вы узнаете, как текст становится числами, как модели находят смысл и почему трансформеры изменили всё."
    },
    "prompt-engineering": {
      "title": "Промпт-инженерия",
      "description": "Освойте искусство общения с ИИ. Узнайте о контекстных окнах, контроле температуры и стратегиях промптинга.",
      "longDescription": "Эффективный промптинг — самый практичный навык работы с ИИ. Этот путь научит вас, как модели читают ваш ввод, как контролировать креативность и структурировать промпты для стабильных результатов."
    },
    "build-rag-apps": {
      "title": "Создание RAG-приложений",
      "description": "Научитесь создавать ИИ-приложения на основе ваших данных. От эмбеддингов до генерации с извлечением.",
      "longDescription": "RAG — самый популярный паттерн для корпоративных ИИ-приложений. Путь охватывает векторные эмбеддинги, управление контекстом, конвейеры извлечения и память."
    },
    "training-deep-dive": {
      "title": "Глубокое погружение в обучение",
      "description": "Поймите, как обучаются модели ИИ — от датасетов и функций потерь до обратного распространения и оценки.",
      "longDescription": "Погрузитесь глубже в то, как модели учатся. Путь охватывает полный конвейер обучения: подготовку данных, функции потерь, обратное распространение, управление эпохами, предотвращение переобучения и оценку модели."
    },
    "career-explorer": {
      "title": "Карьера в ИИ",
      "description": "Откройте для себя карьерные пути в ИИ — от промпт-архитектора до ML-инженера, дата-сайентиста до специалиста по этике ИИ.",
      "longDescription": "ИИ создаёт совершенно новые карьерные пути. Этот маршрут представляет пять ключевых ролей — AI-инженер, промпт-архитектор, дата-сайентист, специалист по этике ИИ и MLOps-специалист."
    }
  },
  "connections": {
    "tabLabel": "Связи",
    "youAreHere": "Вы здесь",
    "positionInTree": "{current} из {total} в {level}",
    "buildsOn": "Основано на",
    "buildsOnEmpty": "Начальная точка — предпосылки не нужны.",
    "unlocks": "Открывает",
    "unlocksEmpty": "Конечная точка — освойте и исследуйте свободно.",
    "sameLevel": "Также в {level}",
    "whyLabel": "Почему?",
    "learnMore": "Узнать больше",
    "conceptLinks": {
      "tokens": {
        "why": "Всё в ИИ начинается с токенов. Нет токенов — нет ИИ.",
        "whyTechnical": "LLM работают с последовательностями токенов, а не с сырым текстом. Токенизация определяет стоимость, скорость и ёмкость контекста."
      },
      "vectors": {
        "why": "Векторы придают словам числовой смысл. Так ИИ «понимает».",
        "whyTechnical": "Эмбеддинги отображают дискретные токены в непрерывные векторные пространства, где семантическое сходство становится геометрической близостью.",
        "fromTokens": "Токены становятся векторами — сырые части текста получают числовой смысл."
      },
      "attention": {
        "why": "Внимание позволяет ИИ фокусироваться на важном в предложении.",
        "whyTechnical": "Self-attention вычисляет попарные оценки релевантности по всем позициям токенов, обеспечивая захват дальних зависимостей.",
        "fromTokens": "Внимание работает с последовательностями токенов для нахождения связей.",
        "fromVectors": "Внимание использует векторные представления для вычисления релевантности между словами."
      },
      "prefill-decode": {
        "why": "Почему ваш вопрос мгновенен, а ответ приходит с задержкой.",
        "whyTechnical": "Prefill обрабатывает входной промпт параллельно, а авторегрессивное декодирование генерирует токены последовательно.",
        "fromTokens": "Обе фазы обрабатывают токены — prefill читает параллельно, decode пишет по одному."
      },
      "context-windows": {
        "why": "Рабочая память ИИ — сколько он может «видеть» за раз.",
        "whyTechnical": "Контекстное окно определяет максимальную длину последовательности (в токенах). Превышение приводит к потере информации.",
        "fromTokens": "Контекстные окна измеряются в токенах — больше токенов = больше памяти."
      },
      "hallucinations": {
        "why": "ИИ может быть уверенно неправ. Знание этого защищает вас.",
        "whyTechnical": "Галлюцинации возникают из авторегрессивного процесса — модели предсказывают статистически вероятные продолжения, а не проверенные факты.",
        "fromTokens": "Модели предсказывают следующий токен по вероятности — а не проверяя факты."
      },
      "transformers": {
        "why": "Архитектура за GPT, Claude и каждой современной моделью ИИ.",
        "whyTechnical": "Трансформеры складывают многоголовое self-attention с feed-forward слоями, обеспечивая параллельную обработку и масштабируемость.",
        "fromAttention": "Многоголовое внимание — основной механизм внутри каждого слоя трансформера.",
        "fromVectors": "Трансформеры обрабатывают эмбеддинги токенов (векторы) через слои внимания."
      },
      "training-vs-inference": {
        "why": "Обучение стоит миллионы. Использование модели — доли цента.",
        "whyTechnical": "Обучение настраивает веса через обратное распространение на массивных данных. Инференс — прямой проход с замороженными весами.",
        "fromTokens": "Обучение обрабатывает триллионы токенов. Инференс обрабатывает ваши токены."
      },
      "temperature-sampling": {
        "why": "Регулятор между «безопасно и предсказуемо» и «творческий хаос».",
        "whyTechnical": "Температура масштабирует распределение логитов перед softmax — низкие значения заостряют к argmax, высокие выравнивают к равномерному.",
        "fromTokens": "Температура контролирует, какой токен будет выбран из распределения вероятностей."
      },
      "prompting-basics": {
        "why": "Самый практичный навык ИИ: задавать лучшие вопросы.",
        "whyTechnical": "Эффективный промптинг использует обучение в контексте, структурируя ввод для активации релевантных способностей модели.",
        "fromTokens": "Каждое слово в промпте — токены, которые модель обрабатывает. Точность экономит стоимость и улучшает вывод."
      },
      "context-engineering": {
        "why": "Не только вопрос — всё окружение для ИИ.",
        "whyTechnical": "Контекстная инженерия систематически конструирует полный ввод: системная роль, ограничения, формат, примеры и запрос.",
        "fromTokens": "Каждая часть спроектированного контекста потребляет токены из бюджета окна."
      },
      "rag": {
        "why": "ИИ, который ищет в ваших документах перед ответом. Обоснованный, а не угадывающий.",
        "whyTechnical": "RAG: (1) эмбеддинг запроса, (2) извлечение Top-K через векторный поиск, (3) дополнение промпта, (4) генерация обоснованного ответа.",
        "fromVectors": "RAG использует векторный поиск для нахождения релевантных документов по смыслу, а не по ключевым словам.",
        "fromMemory": "RAG расширяет краткосрочный контекст долгосрочным извлечением документов."
      },
      "memory": {
        "why": "Краткосрочная (разговор) vs долгосрочная (база данных). ИИ нужны обе.",
        "whyTechnical": "Краткосрочная: ограничена токенами контекстного окна. Долгосрочная: внешние векторные хранилища с семантическим поиском.",
        "fromVectors": "Долгосрочная память хранит и извлекает информацию как векторы для семантического поиска."
      },
      "lora": {
        "why": "Настройте модель под свою область за 1% стоимости.",
        "whyTechnical": "LoRA внедряет обучаемые матрицы низкого ранга рядом с замороженными весами. Обучение ~0.1% параметров достигает результатов, близких к полному дообучению.",
        "fromVectors": "Адаптеры LoRA изменяют то, как модель преобразует входные векторы.",
        "fromAttention": "LoRA обычно нацелена на весовые матрицы внимания (проекции Q, K, V)."
      },
      "security": {
        "why": "Входные атаки, отравление модели, выходные риски. Три уровня защиты.",
        "whyTechnical": "Глубокая защита: валидация входа, укрепление модели (выравнивание, RLHF), фильтрация выхода.",
        "fromTokens": "Атаки инъекции промптов эксплуатируют то, как модели обрабатывают входные токены."
      },
      "ai-agents": {
        "why": "ИИ, который не просто отвечает — он действует. Наблюдать, думать, выполнять.",
        "whyTechnical": "Агенты реализуют цикл наблюдение-рассуждение-действие через вызовы инструментов и итерацию до завершения задачи.",
        "fromContextEngineering": "Агентам нужны тщательно спроектированные системные промпты для определения роли, инструментов и границ.",
        "fromRag": "Агенты используют RAG для доступа к базам знаний."
      },
      "mcp": {
        "why": "Один протокол для подключения любого ИИ к любому инструменту. Как USB-C для ИИ.",
        "whyTechnical": "MCP стандартизирует интерфейс ИИ-инструмент: обнаружение, схемы параметров, аутентификация и выполнение.",
        "fromAiAgents": "MCP обеспечивает стандартный интерфейс, который агенты используют для подключения к внешним инструментам."
      },
      "complexity-levels": {
        "why": "LLM → Модель рассуждений → Агент. Три уровня возможностей ИИ.",
        "whyTechnical": "Прогрессия: (1) однопроходное предсказание, (2) многошаговые цепочки рассуждений, (3) автономные агенты с инструментами и памятью.",
        "fromTokens": "Все три уровня обрабатывают токены — разница в количестве шагов рассуждения."
      },
      "function-calling": {
        "why": "Даём ИИ руки — он может вызывать API, запрашивать базы данных, отправлять письма.",
        "whyTechnical": "Модель генерирует структурированный JSON вызова функции. Приложение выполняет вызов, возвращает результаты.",
        "fromAiAgents": "Вызов функций — механизм взаимодействия агентов с реальным миром."
      },
      "moe": {
        "why": "8 моделей-экспертов, но активны только 2 на запрос. Быстро и дёшево.",
        "whyTechnical": "Разреженный MoE направляет каждый токен через обученную сеть маршрутизации к K из N экспертных подсетей.",
        "fromAttention": "Каждый эксперт применяет свои слои внимания и feed-forward к направленным токенам.",
        "fromVectors": "Сеть маршрутизации использует векторные представления для выбора лучших экспертов."
      },
      "reasoning-models": {
        "why": "ИИ, который думает пошагово перед ответом. Медленнее, но надёжнее.",
        "whyTechnical": "Модели рассуждений генерируют промежуточные токены рассуждений, самопроверку и откат перед финальным ответом.",
        "fromAttention": "Расширенное рассуждение требует обработки гораздо большего числа шагов внимания.",
        "fromContextEngineering": "Модели рассуждений выигрывают от хорошо структурированных промптов."
      },
      "green-ai": {
        "why": "Такое же качество ИИ, на 90% меньше энергии. Эффективность — следующий рубеж.",
        "whyTechnical": "Техники: дистилляция модели, квантизация, эффективные архитектуры (MoE) и оптимизированный инференс.",
        "fromTokens": "Обработка меньшего числа токенов и использование меньших моделей резко снижает потребление энергии.",
        "fromAttention": "Эффективные варианты внимания (flash attention, sparse attention) — ключ к Green AI."
      },
      "agi-asi": {
        "why": "От узкого ИИ (сегодня) к общему интеллекту (будущее?) и далее.",
        "whyTechnical": "ОИИ: производительность на уровне человека во всех когнитивных областях. СИИ: превосходство во всех измерениях.",
        "fromAiAgents": "Агенты представляют ближайший шаг к общим возможностям — автономное многоинструментальное рассуждение.",
        "fromAttention": "Масштабирование механизмов внимания — один из путей к более общему интеллекту."
      }
    }
  },
  "upNext": {
    "title": "Далее",
    "sameLevel": "Продолжить на этом уровне",
    "nextLevel": "Следующий уровень",
    "prerequisiteFor": "Открывает",
    "allDone": "Вы исследовали все концепции!",
    "allDoneDesc": "Отлично! Просматривайте дерево, чтобы вернуться к любой концепции.",
    "browseTree": "Просмотреть дерево"
  },
  "celebration": {
    "levelComplete": "Уровень Пройден!",
    "levelCompleteDesc": "Вы прошли все {count} концепций уровня {level}.",
    "allComplete": "Дерево Пройдено!",
    "allCompleteDesc": "Вы изучили все {count} концепций. Вы выросли от корней до листьев!",
    "nextLevel": "Продолжить к {level}",
    "shareProgress": "Поделиться прогрессом",
    "keepExploring": "Продолжить исследование",
    "awesome": "Отлично!",
    "certificate": "Сертификат об окончании"
  },
  "dna": {
    "header": {
      "title": "Механизм",
      "subtitle": "Каждая мысль ИИ следует одному и тому же 4-шаговому пути. От сырого текста к смыслу и обратно."
    },
    "input": {
      "placeholder": "Введите что-нибудь (напр. «Почему небо голубое?»)",
      "interactiveMode": "ИНТЕРАКТИВНЫЙ РЕЖИМ",
      "status": "СТАТУС: {step}...",
      "reset": "Сбросить симуляцию",
      "confirmReset": "Нажмите ещё раз для сброса"
    },
    "card": {
      "deepDive": "ПОГРУЖЕНИЕ →",
      "learnMore": "Подробнее →",
      "helpTooltip": "Что это за шаг?"
    },
    "steps": {
      "complete": {
        "tokenization": "Текст токенизирован! Каждый кусочек имеет уникальный ID.",
        "vectorizing": "Токены векторизованы! Похожие слова группируются.",
        "attention": "Внимание рассчитано! Смотрите, какие слова связаны.",
        "prediction": "Предсказание выполнено! Проверьте лучшего кандидата."
      },
      "hint": {
        "attention": "Нажмите на токен, чтобы увидеть его связи"
      }
    },
    "nav": {
      "next": "Далее",
      "finish": "Завершить",
      "done": "Готово",
      "play": "Запустить симуляцию",
      "pause": "Приостановить симуляцию",
      "resume": "Продолжить симуляцию",
      "nextStep": "Следующий шаг",
      "playing": "Воспроизведение",
      "paused": "Пауза",
      "stepOf": "Шаг {current} из {total}",
      "stepName": {
        "tokenization": "Токенизация",
        "vectorizing": "Векторизация",
        "attention": "Внимание",
        "prediction": "Предсказание"
      },
      "thicknessLabel": "толщина = важность",
      "tapToSpotlight": "нажмите на слово для выделения связей",
      "connections": "{count} связей"
    },
    "seed": {
      "growing": "Растём в Дерево...",
      "exploring": "Исследуем глубже..."
    },
    "microLesson": {
      "metaphorLabel": "Метафора",
      "resumeFlow": "Продолжить поток",
      "exploreMore": "Исследовать больше",
      "tokenization": {
        "title": "Как ИИ читает",
        "body": "Текст рассыпается на крошечные числа — единственный язык, понятный машине.",
        "metaphor": "Как нарезать предложение на кубики LEGO."
      },
      "vectorizing": {
        "title": "Слова становятся числами",
        "body": "Каждое слово получает GPS-координату во вселенной смыслов. Похожие понятия приземляются рядом.",
        "metaphor": "Как дать каждому слову GPS-координату во вселенной смысла."
      },
      "attention": {
        "title": "Соединение точек",
        "body": "Прожектор освещает комнату, находя слова, которые наиболее важны друг для друга.",
        "metaphor": "Как прожектор в тёмной комнате, освещающий связи."
      },
      "prediction": {
        "title": "Угадывание следующего",
        "body": "Основываясь на всём изученном, модель предсказывает, что будет дальше. По одному слову.",
        "metaphor": "Как угадать конец предложения до того, как оно произнесено."
      }
    },
    "completion": {
      "title": "Симуляция завершена",
      "subtitle": "Вы исследовали 4-шаговый путь от текста к предсказанию.",
      "predicted": "Лучшее предсказание",
      "replay": "Повторить шаги",
      "explore": "Посадить семя"
    },
    "controlHint": "Двигайте курсор для замедления · Нажимайте на карточки для исследования",
    "seedPath": {
      "title": "Посадите Своё Семя",
      "subtitle": "Выберите путь к пониманию ИИ",
      "builder": {
        "label": "Строитель",
        "description": "Хочу создавать ИИ-приложения"
      },
      "thinker": {
        "label": "Мыслитель",
        "description": "Хочу понять ИИ"
      },
      "explorer": {
        "label": "Исследователь",
        "description": "Просто смотрю"
      }
    },
    "mobile": {
      "swipeHint": "Свайпните для навигации по шагам",
      "cardOf": "Шаг {current} из {total}",
      "tapToExplore": "Нажмите на карточку для исследования"
    },
    "orientation": {
      "title": "Как это работает",
      "instruction": "Введите любой текст выше и нажмите ▶, чтобы увидеть, как ИИ его читает.",
      "description": "Наблюдайте, как ваши слова трансформируются через 4 шага: T → V → A → P",
      "example": "Попробуйте: «Король носил корону»",
      "tapToTry": "Нажмите, чтобы попробовать этот пример"
    },
    "accordion": {
      "locked": "заблокировано",
      "next": "Далее",
      "deeper": "Глубже",
      "resume": "Продолжить",
      "goToSeed": "Исследовать в Семени",
      "tapToSkip": "Далее",
      "holdToPause": "Удерживайте для паузы",
      "holdingPaused": "Отпустите для продолжения",
      "paused": "Пауза"
    },
    "summary": {
      "tokenization": "Текст разбит на {count} частей",
      "vectorizing": "Отображено на {count} координат",
      "attention": "Найдено {count} связей",
      "prediction": "Победитель: «{token}» ({percent}%)"
    }
  },
  "programs": {
    "catalog": {
      "title": "Выберите Путь",
      "subtitle": "От понимания механизма до создания автоматизации. Выберите программу, соответствующую вашим целям.",
      "duration": "Длительность",
      "weeks": "Недели",
      "effort": "Нагрузка",
      "totalHours": "ч Всего",
      "viewDetails": "Подробнее",
      "bestValue": "Лучшее предложение",
      "compare": "Сравнить программы",
      "compareFeature": "Функция",
      "comparePrice": "Цена",
      "compareDuration": "Длительность",
      "compareHours": "Часы",
      "compareLevel": "Уровень"
    },
    "hero": {
      "apply": "Подать заявку",
      "weeks": "недель",
      "hours": "часов"
    },
    "features": {
      "heading": "Чему вы научитесь"
    },
    "curriculum": {
      "heading": "Программа по неделям",
      "subtitle": "Структурированный путь от основ до мастерства",
      "week": "Неделя"
    },
    "pricing": {
      "heading": "Инвестируйте в",
      "headingAccent": "Своё Будущее",
      "benefits": [
        "Пожизненный доступ к материалам курса",
        "Сертификат об окончании",
        "Доступ к сообществу и нетворкинг",
        "Практические проекты и упражнения"
      ],
      "guarantee": "30-дневная гарантия",
      "guaranteeDesc": "Не устраивает? Полный возврат в течение 30 дней, без вопросов.",
      "totalInvestment": "Стоимость",
      "vatNote": "НДС не включён",
      "flexiblePayment": "Гибкая оплата",
      "cta": "Записаться",
      "paymentNote": "Безопасная оплата. Начните учиться немедленно.",
      "graduateDiscount": "Скидка выпускникам"
    },
    "faq": {
      "heading": "Часто задаваемые вопросы"
    },
    "lead": {
      "title": "Забронируйте место",
      "subtitle": "Оставьте свои данные, и мы свяжемся с вами.",
      "name": "Полное имя",
      "namePlaceholder": "Иван Иванов",
      "email": "Электронная почта",
      "emailPlaceholder": "ivan@example.com",
      "phone": "Телефон (необязательно)",
      "phonePlaceholder": "+7 ...",
      "notes": "Цели или вопросы (необязательно)",
      "notesPlaceholder": "Что вы хотите получить от этой программы?",
      "submit": "Забронировать место",
      "submitting": "Отправка...",
      "successTitle": "Добро пожаловать!",
      "successMessage": "Мы получили вашу заявку. Проверьте почту — мы свяжемся в течение 24 часов.",
      "successClose": "Понятно",
      "errorGeneric": "Что-то пошло не так. Попробуйте ещё раз.",
      "errorEmail": "Введите корректный адрес электронной почты.",
      "errorName": "Введите ваше имя.",
      "close": "Закрыть"
    }
  },
  "nav": {
    "tree": "Дерево",
    "mechanism": "Механизм",
    "learn": "Программы",
    "proto": "Прото",
    "back": "Назад",
    "settings": "Настройки"
  },
  "stages": {
    "dna": "ДНК",
    "seed": "Семя",
    "sprout": "Росток",
    "sapling": "Саженец",
    "tree": "Дерево",
    "fruits": "Плоды",
    "orchard": "Сад",
    "sub": {
      "dna": "Механизм",
      "seed": "Истоки",
      "sprout": "Зарождение",
      "sapling": "Питомник",
      "tree": "Знания",
      "fruits": "Применения",
      "orchard": "Карьера"
    },
    "description": {
      "dna": "Механизм — Как ИИ обрабатывает текст",
      "seed": "Ваше намерение — Что вы хотите исследовать?",
      "sprout": "Основы — 6 ключевых концепций ИИ",
      "tree": "Знания — Глубокое погружение во все концепции",
      "fruits": "Применения — Что ИИ может сделать для вас?",
      "orchard": "Урожай карьеры — Профессиональные пути в ИИ"
    }
  },
  "floatingInput": {
    "placeholder": "Спросите что угодно об ИИ...",
    "expand": "Развернуть ввод",
    "collapse": "Свернуть ввод",
    "submit": "Спросить"
  },
  "fruits": {
    "title": "Урожай",
    "subtitle": "Где интеллект приносит плоды. Исследуйте практические применения и ценность, создаваемую системой ИИ.",
    "phaseLabel": "Фаза III: Применение",
    "inputPlaceholder": "Поиск приложений...",
    "card": {
      "visitApp": "Запустить"
    }
  },
  "seed": {
    "title": "Обучение",
    "subtitle": "Как сырые данные становятся интеллектуальной моделью.",
    "phaseLabel": "Фаза I: Семя",
    "inputPlaceholder": "Запросить данные обучения...",
    "motivator": "Зачем это учить?",
    "pathPractical": "Чтобы писать лучшие промпты, нужно знать, как модель 'читает' данные.",
    "pathCareer": "Курация обучающих данных — ценный навык для AI-инженеров.",
    "pathCurious": "Поймите 'черный ящик' — как цифры становятся знаниями.",
    "card": {
      "learnMore": "Анализ процесса"
    },
    "steps": {
      "dataset": {
        "title": "Датасет",
        "desc": "Сырая информация, собранная из мира. Это почва."
      },
      "training": {
        "title": "Обучение (Сжатие)",
        "desc": "Интенсивный процесс сжатия данных в веса. Максимизация распознавания паттернов."
      },
      "model": {
        "title": "Модель",
        "desc": "Финальный сжатый артефакт. Статичный файл, готовый к «пробуждению»."
      }
    },
    "sections": {
      "dataset": "1. Датасет",
      "training": "2. Обучение",
      "model": "3. Модель"
    },
    "hero": {
      "selectData": "Выбрать данные обучения",
      "selectDataHelp": "Выберите хотя бы один датасет, чтобы начать обучение.",
      "ingestData": "Загрузить данные",
      "compressing": "Сжатие знаний...",
      "epoch": "Эпоха",
      "loss": "Потери",
      "modelReady": "Базовая модель готова",
      "modelReadyDesc": "Знания сжаты. Веса оптимизированы. Модель готова к файн-тюнингу.",
      "finalLoss": "Финальная ошибка",
      "parameters": "Параметры"
    },
    "nav": {
      "ingestion": "Загрузка",
      "training": "Обучение",
      "model": "Модель",
      "resetRun": "Сбросить запуск"
    }
  },
  "orchard": {
    "title": "Карьерные Пути",
    "subtitle": "Выращивайте своё будущее. Исследуйте профессиональные возможности в экосистеме ИИ.",
    "phaseLabel": "Фаза IV: Сад",
    "inputPlaceholder": "Введите свои навыки...",
    "yourPath": "Ваш Путь?",
    "growingField": "Отрасль растёт с каждым днём.",
    "card": {
      "viewPath": "Карьерный путь"
    }
  },
  "sapling": {
    "phaseLabel": "Питомник",
    "title": "Питомник",
    "subtitle": "Где абстрактные модели становятся реальными. Экспериментируйте и убедитесь сами.",
    "keyConcepts": "Ключевые Концепции",
    "trainingModules": "Учебные Модули",
    "moduleActive": "Активен",
    "moduleStart": "Начать модуль",
    "modules": {
      "basics": {
        "title": "Первый Промпт",
        "description": "Изучите причинно-следственную связь базового промптинга.",
        "promptTemplate": "Напиши короткое стихотворение о роботе, который учится садоводству."
      },
      "refinement": {
        "title": "Улучшение",
        "description": "Итерируйте промпт для лучших результатов.",
        "promptTemplate": "Напиши смешное хайку о роботе-садовнике, который любит морковку."
      },
      "temperature": {
        "title": "Температура",
        "description": "Экспериментируйте со случайностью и креативностью.",
        "promptTemplate": "Придумай 5 новых названий для футуристического фрукта."
      },
      "evaluation": {
        "title": "Оценка",
        "description": "Оцените качество и безопасность выводов ИИ.",
        "promptTemplate": "Объясни квантовую физику пятилетнему ребёнку с помощью эмодзи."
      }
    }
  },
  "promptSandbox": {
    "yourPrompt": "Ваш Промпт",
    "temperature": "Креативность (Температура)",
    "placeholder": "Введите промпт...",
    "runPrompt": "Запустить промпт",
    "iterations": "Итерации",
    "aiOutput": "Вывод ИИ",
    "readyMessage": "Готов к экспериментам",
    "emptyHint": "Введите запрос ниже и нажмите Отправить, чтобы увидеть ответ ИИ",
    "qualityScore": "Оценка связности:",
    "ratePositive": "Оценить ответ положительно"
  },
  "conceptDetail": {
    "metaphor": "Метафора",
    "deepDive": "Погружение",
    "thinkAboutIt": "Подумайте об этом"
  },
  "relatedConcepts": {
    "title": "Связанные концепции"
  },
  "treeExplorer": {
    "map": "Карта",
    "grid": "Сетка"
  },
  "seedHero": {
    "initializeWeights": "Инициализировать веса",
    "weightsExplanation": "«Веса» — как регуляторы громкости. Обучение настраивает миллиарды из них, чтобы уменьшить шум и усилить смысл.",
    "startTrainingLoop": "Запустить цикл обучения",
    "output": {
      "chaos": "x7zn# m.q9^2 @lp! s_?k...",
      "broken": "the cat s@t on... {chunk}... predict_nxt",
      "fluent": "\"The neural network is learning to generate coherent text structure.\"",
      "final": "\"The neural network has successfully learned to generate coherent text structure from the training data.\""
    }
  },
  "stageIntro": {
    "dna": "Загляните внутрь ИИ.",
    "seed": "Откуда берутся знания.",
    "sprout": "Что возникает из обучения.",
    "sapling": "Попробуйте сами — безопасно.",
    "tree": "Углубляйтесь, когда будете готовы.",
    "fruits": "Постройте что-то настоящее.",
    "orchard": "Сделайте это своей профессией."
  },
  "stageQuestion": {
    "dna": "Как работает одно предсказание?",
    "seed": "Откуда ИИ берёт знания?",
    "sprout": "Чем становится обученная модель?",
    "sapling": "Смогу ли я сделать это сам?",
    "tree": "Насколько глубоко это уходит?",
    "fruits": "Что я могу построить с этим?",
    "orchard": "Куда это меня приведёт?"
  }
}