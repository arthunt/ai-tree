{
  "metadata": {
    "title": "AI Knowledge Tree | AI Teadmiste Puu",
    "description": "Comprehensive interactive framework for teaching and understanding AI concepts. Terviklik interaktiivne raamistik AI kontseptide √µpetamiseks ja m√µistmiseks."
  },
  "header": {
    "title": "AI Knowledge Tree",
    "description": "Master AI fundamentals in ~2 hours",
    "treeView": "Concept Map",
    "treeViewAriaLabel": "Go to concept map view",
    "classicView": "Learning Path",
    "classicViewAriaLabel": "Back to learning path view"
  },
  "hero": {
    "title": "AI Knowledge Tree",
    "subtitle": "Understand AI well enough to make informed decisions. Master the fundamentals in ~2 hours.",
    "forWhom": "For trainers, team leads, and curious professionals",
    "classicViewTitle": "Learning Path",
    "classicViewDescription": "Step-by-step progression through concepts",
    "currentPage": "You are here",
    "treeViewTitle": "Concept Map",
    "treeViewDescription": "Visual overview of all concepts",
    "clickHere": "Click here ‚Üí",
    "or": "or",
    "treeEmoji": "Tree emoji",
    "bookEmoji": "Book",
    "startFromRoots": "Start from roots and move up or scroll freely",
    "newToAI": "New to AI?",
    "beginnerPathDesc": "Start here! Understand the basics in ~30 minutes.",
    "startWithTokens": "Start with Tokens",
    "startLearning": "Start Learning",
    "hook": "You already use AI. Now understand how it works.",
    "showMore": "Show more",
    "showLess": "Show less"
  },
  "levels": {
    "roots": "Roots",
    "trunk": "Trunk",
    "branches": "Branches",
    "leaves": "Leaves",
    "totalTime": "Complete path: ~2 hours",
    "levelTime": {
      "roots": "~25 min",
      "trunk": "~30 min",
      "branches": "~35 min",
      "leaves": "~30 min"
    }
  },
  "treeView": {
    "title": "AI Knowledge Tree - Concept Map",
    "description": "All concepts in one view",
    "instructionsTitle": "Interactive Concept Map",
    "instructionsText": "Hover over a concept to highlight it. Click to open detailed view.",
    "ariaLabel": "Concept map",
    "diagramAriaLabel": "AI concepts map diagram",
    "legendHeading": "Tree levels legend",
    "rootsEmoji": "Seedling emoji",
    "trunkEmoji": "Evergreen tree emoji",
    "branchesEmoji": "Green plant emoji",
    "leavesEmoji": "Leaves emoji",
    "rootsLabel": "1. Roots",
    "trunkLabel": "2. Trunk",
    "branchesLabel": "3. Branches",
    "leavesLabel": "4. Leaves"
  },
  "viewMode": {
    "both": "Both",
    "simple": "Simple",
    "technical": "Technical",
    "ariaLabel": "Content mode selection",
    "tooltip": {
      "simple": "Simple metaphors and everyday examples",
      "technical": "Technical explanations for developers",
      "both": "Show both side by side"
    }
  },
  "nameToggle": {
    "simple": "Simple names",
    "technical": "Technical names",
    "ariaLabel": "Name display selection"
  },
  "darkMode": {
    "toggle": "Dark mode",
    "ariaLabel": "Dark/light mode selection"
  },
  "settings": {
    "ariaLabel": "Open settings menu",
    "viewMode": "View Mode",
    "theme": "Theme",
    "language": "Language"
  },
  "footer": {
    "description": "AI Knowledge Tree ‚Äì Interactive learning tool for understanding AI concepts",
    "version": "Version",
    "reportIssue": "Report Issue"
  },
  "navigation": {
    "viewOptions": "View options",
    "levelSections": "AI concept levels"
  },
  "complexity": {
    "level": "Complexity level",
    "beginner": "Beginner",
    "intermediate": "Intermediate",
    "advanced": "Advanced"
  },
  "loading": {
    "default": "Loading...",
    "concepts": "Loading concepts...",
    "tree": "Loading tree...",
    "content": "Loading content..."
  },
  "concept": {
    "simpleName": "Simple name",
    "explanation": "Explanation",
    "metaphor": "Metaphor",
    "closeAriaLabel": "Close detailed view",
    "closeDialog": "Close dialog window",
    "viewDetails": "View details",
    "viewFullSize": "View full size",
    "beginnerPath": "Beginner Path",
    "readingTime": "min read",
    "learnFirst": "Learn first",
    "prerequisiteHelp": "Understanding these topics will help you better grasp this concept:",
    "navigateTo": "Navigate to topic",
    "simpleMetaphor": "Simple Metaphor",
    "technicalExplanation": "Technical Explanation",
    "codeExample": "Code Example",
    "pressEscToClose": "Press ESC or click outside to close",
    "backToTree": "Back to Tree",
    "share": "Share this concept",
    "shareNative": "Share",
    "shareTwitter": "Share on X (Twitter)",
    "shareLinkedIn": "Share on LinkedIn",
    "copyLink": "Copy link",
    "linkCopied": "Link copied!",
    "completed": "Completed",
    "markAsComplete": "Mark as understood",
    "markedComplete": "Understood!",
    "conceptMarkedComplete": "Concept marked as complete!",
    "undo": "Undo",
    "simpleMode": "Simple",
    "technicalMode": "Technical",
    "explanation": "Explanation",
    "visual": "Visual",
    "code": "Code",
    "whyThisCode": "Why this code?",
    "howToUse": "How to use",
    "tryItOut": "Try it out",
    "visualComingSoon": "Visual representation coming soon",
    "visualComingSoonDesc": "We're working on adding visual content for this concept.",
    "codeComingSoon": "Code example coming soon",
    "codeComingSoonDesc": "We're preparing a practical code example for this concept.",
    "swipeHint": "Swipe left/right for prev/next concept",
    "copyCode": "Copy code",
    "codeCopied": "Code copied!"
  },
  "tokenizer": {
    "title": "Tokenizer Demo",
    "subtitle": "See how AI sees text",
    "showInfo": "Show info",
    "hideInfo": "Hide info",
    "inputLabel": "Enter text to tokenize",
    "inputPlaceholder": "Write something to see how AI tokenizes text...",
    "tokensLabel": "Tokens:",
    "tokenCount": "{count} {count, plural, one {token} other {tokens}}",
    "tokensPlaceholder": "Tokens will appear here...",
    "infoTitle": "What are tokens?",
    "infoText": "Language models don't see text the way we do. They break text into smaller pieces called <em>tokens</em>. A token can be a word, part of a word, or even a single character. This demo shows a simplified version!",
    "estimatedCost": "Estimated cost (GPT-4)",
    "costTip": "<strong>Tip:</strong> Longer texts = more tokens = higher cost. Learn to optimize your prompts!",
    "tryExamples": "Try examples:",
    "digitInsight": "<strong>Numbers split into digits!</strong> AI doesn't see \"2024\" as one number ‚Äî it becomes 4 separate tokens (2, 0, 2, 4). Each digit gets its own <em>vector</em> (numerical meaning). That's why AI can struggle with math ‚Äî it processes digits one by one, not as whole numbers.",
    "subwordInsight": "<strong>Long words get split!</strong> Tokens marked with <code>##</code> are subword pieces. AI breaks unfamiliar or long words into smaller known parts, similar to how you might sound out an unfamiliar word syllable by syllable.",
    "disclaimer": "This is a simplified tokenizer. Real language models use more sophisticated methods like BPE (Byte Pair Encoding) or WordPiece."
  },
  "navigation": {
    "viewOptions": "View options",
    "levelSections": "AI concept levels",
    "treeLevels": "Tree level navigation",
    "openNavPanel": "Open navigation panel",
    "closeNavPanel": "Close navigation panel",
    "goToLevel": "Go to {level} level",
    "currentLevel": "Current",
    "conceptsCompleted": "concepts understood",
    "backToTop": "Back to top",
    "switchToEstonian": "Switch to Estonian",
    "switchToEnglish": "Switch to English"
  },
  "journey": {
    "title": "Your Learning Path",
    "subtitle": "From everyday AI to deep understanding ‚Äî like growing a tree from roots to leaves.",
    "roots": "Foundation",
    "rootsDesc": "Tokens, vectors & the building blocks",
    "trunk": "Core Mechanics",
    "trunkDesc": "How models learn and generate",
    "branches": "Applications",
    "branchesDesc": "RAG, agents & real-world tools",
    "leaves": "Mastery",
    "leavesDesc": "Fine-tuning, safety & beyond"
  },
  "levelSection": {
    "conceptCount": "{count} concepts",
    "concepts": "{level} concepts"
  },
  "organicTree": {
    "clickForDetails": "Click for details",
    "simple": "Simple",
    "intermediate": "Intermediate",
    "advanced": "Advanced"
  },
  "search": {
    "button": "Search",
    "buttonLabel": "Open search (Cmd+K)",
    "placeholder": "Search concepts...",
    "inputLabel": "Search for AI concepts",
    "instructions": "Use arrow keys to navigate, Enter to select, and Escape to close",
    "close": "Close search",
    "noResults": "No concepts found",
    "tryDifferent": "Try a different search term",
    "recentSearches": "Recent Searches",
    "clearRecent": "Clear",
    "popularConcepts": "Popular Concepts",
    "navigate": "Navigate",
    "select": "Select",
    "resultsCount": "{count} {count, plural, one {result} other {results}}",
    "level": {
      "roots": "Roots",
      "trunk": "Trunk",
      "branches": "Branches",
      "leaves": "Leaves"
    }
  },
  "skillSelector": {
    "title": "Where should we start?",
    "subtitle": "Choose your experience level",
    "close": "Close modal",
    "beginner": {
      "title": "New to AI",
      "description": "Start from the basics. Learn what tokens, vectors, and transformers are.",
      "time": "~30 min to get started"
    },
    "intermediate": {
      "title": "Exploring AI",
      "description": "Know the basics? Dive into how models learn and generate content.",
      "time": "~45 min deeper dive"
    },
    "advanced": {
      "title": "Building with AI",
      "description": "Ready for advanced topics like fine-tuning, RAG, and agents.",
      "time": "~35 min advanced topics"
    },
    "skipAndExplore": "Skip and explore freely"
  },
  "vectorDemo": {
    "title": "Vector Similarity Demo",
    "subtitle": "Explore how AI understands word meanings",
    "showInfo": "Show info",
    "hideInfo": "Hide info",
    "reset": "Reset",
    "word": "Word",
    "optional": "(optional)",
    "wordPlaceholder": "e.g. cat, happy, king",
    "calculate": "Calculate Similarity",
    "similarityScores": "Similarity Scores:",
    "similarityHigh": "Very similar",
    "similarityMedium": "Somewhat similar",
    "similarityLow": "Not similar",
    "visualization": "2D Visualization:",
    "visualizationAriaLabel": "2D visualization of word similarities in vector space",
    "visualizationHint": "Words closer together are more similar in meaning. Hover over points for details.",
    "infoTitle": "What are embeddings?",
    "infoText": "AI represents words as vectors (lists of numbers) called <em>embeddings</em>. Similar words have similar vectors. This demo uses simplified 10-dimensional vectors, but real models like GPT use 1,536+ dimensions! The 2D plot shows these high-dimensional relationships projected into a space we can see.",
    "tryExamples": "Try examples:",
    "disclaimer": "This demo uses pre-computed simplified embeddings. Real AI models create embeddings dynamically based on context and use thousands of dimensions for more nuanced understanding."
  },
  "codeBlock": {
    "copied": "Copied!",
    "copiedAriaLabel": "Copied!",
    "copy": "Copy",
    "copyAriaLabel": "Copy code",
    "explanation": "üí° Explanation: "
  },
  "quickJump": {
    "ariaLabel": "Scroll down and start learning",
    "startLearning": "Start learning"
  },
  "skeleton": {
    "loadingTree": "Loading tree..."
  },
  "toast": {
    "closeAriaLabel": "Close notification"
  },
  "visuals": {
    "attention": {
      "title": "Attention Mechanism ‚Äî Focus",
      "inputSentence": "Input sentence:",
      "word": {
        "went": "went",
        "store": "to store",
        "and": "and",
        "she": "she",
        "bought": "bought",
        "milk": "milk"
      },
      "processing": "When processing \"she\" (it/she/he):",
      "strong": "Strong (95%) ‚Äî refers to Mari",
      "medium": "Medium (72%)",
      "explanationTitle": "Attention shows which words are important for understanding each word",
      "explanationLine1": "\"she\" pays strong attention to \"Mari\" to know who is being referred to.",
      "explanationLine2": "Arrow thickness = attention strength. This helps the model understand relationships."
    },
    "prefillDecode": {
      "title": "Prefill vs Decode ‚Äî Reading vs Writing",
      "prefillTitle": "PREFILL ‚Äî Reading",
      "decodeTitle": "DECODE ‚Äî Writing",
      "token": {
        "write": "Write",
        "me": "me",
        "a": "a",
        "story": "story",
        "once": "Once",
        "upon": "upon"
      },
      "allAtOnce": "All processed at once",
      "fastParallel": "‚ö° Fast & Parallel",
      "prefillSpeed": "~50ms for 100 tokens",
      "oneAtATime": "One token at a time",
      "sequential": "üê¢ Sequential",
      "decodeSpeed": "~50ms per token",
      "decodeExample": "(100 tokens = 5 seconds)",
      "whyMatters": "Why This Matters",
      "explanationLine1": "Prefill processes your entire prompt instantly (parallel). Decode generates each new word one-by-one (sequential).",
      "explanationLine2": "This is why long responses take time, even though reading your question is instant."
    },
    "contextWindow": {
      "title": "Context Window ‚Äî Working Memory",
      "tokenFlow": "Token Flow:",
      "forgotten": "Forgotten",
      "pastLimit": "Past limit",
      "activeContext": "ACTIVE CONTEXT",
      "modelCanSee": "Model can \"see\" and use this",
      "available": "Available",
      "spaceLeft": "Space left",
      "flowDirection": "Tokens flow through window ‚Üí",
      "sizesByModel": "Context Window Sizes by Model",
      "gpt35Size": "4K tokens",
      "gpt4Size": "8K tokens",
      "gpt4TurboSize": "128K tokens",
      "claudeSize": "200K tokens",
      "visualScale": "Visual scale (1K = 1px width):",
      "explanation": "Think of it like RAM: larger context = can remember more of your conversation, but costs more and runs slower."
    },
    "hallucinations": {
      "title": "Hallucinations ‚Äî Confident Fabrications",
      "subtitle": "Both responses look equally confident, but one is completely wrong!",
      "hallucination": "‚ùå HALLUCINATION",
      "correct": "‚úì CORRECT",
      "question": "Q: Who built the Eiffel Tower?",
      "answerStart": "A: The Eiffel Tower was built in",
      "wrongFact": "1887 by Claude Monet",
      "correctFact": "1889 by Gustave Eiffel",
      "answerEnd": "as part of the World's Fair in Paris.",
      "confidence94": "Confidence: 94%",
      "confidence96": "Confidence: 96%",
      "problem": "‚ö†Ô∏è The Problem: Same confidence level!",
      "whyTitle": "Why Hallucinations Happen",
      "reason1": "‚Ä¢ LLMs generate text based on patterns, not facts from a database",
      "reason2": "‚Ä¢ They don't know when they're making things up ‚Äî they just predict likely next words",
      "reason3": "‚Ä¢ High confidence ‚â† accurate. The model can be very confident about wrong information",
      "tip": "‚úì Always verify important facts, especially dates, names, and technical details!"
    },
    "trainingInference": {
      "title": "Training vs Inference ‚Äî School vs Work",
      "trainingTitle": "TRAINING",
      "trainingPhase": "(Learning Phase)",
      "inferenceTitle": "INFERENCE",
      "inferencePhase": "(Using Phase)",
      "time": "Time",
      "cost": "Cost",
      "data": "Data",
      "modelWeights": "Model Weights",
      "trainingTime": "Weeks to Months",
      "trainingCost": "$2M - $100M+",
      "trainingGpus": "1,000s - 10,000s",
      "trainingData": "Terabytes",
      "weightsChanging": "CHANGING ‚ö°",
      "trainingDesc": "Learns patterns from billions of examples",
      "inferenceTime": "Milliseconds",
      "inferenceCost": "$0.002 per query",
      "inferenceGpus": "1 - 8",
      "inferenceData": "Your prompt",
      "weightsFrozen": "FROZEN üîí",
      "inferenceDesc": "Uses learned patterns to answer your questions",
      "explanation": "Training happens once (expensive). Inference happens millions of times per day (cheap & fast)."
    },
    "transformers": {
      "title": "Transformers ‚Äî Master Architecture",
      "subtitle": "The architecture behind GPT, Claude, Llama, and most modern AI models",
      "inputText": "INPUT TEXT",
      "tokenEmbedding": "Token Embedding",
      "blocksLabel": "TRANSFORMER BLOCKS √ó N (e.g., 12-96 layers)",
      "multiHeadAttention": "Multi-Head Attention üîç",
      "feedForward": "Feed-Forward + Layer Norm",
      "repeatedN": "(repeated N times)",
      "outputProbs": "Output Probabilities",
      "nextToken": "NEXT TOKEN",
      "annotationAttention1": "‚Üê Where words look",
      "annotationAttention2": "at each other",
      "annotationFF1": "‚Üê Processing &",
      "annotationFF2": "transformation",
      "annotationOutput1": "‚Üê Predicts next word",
      "annotationOutput2": "probabilities",
      "keyInnovation": "Key Innovation",
      "innovation1": "‚Ä¢ Parallel processing",
      "innovation2": "‚Ä¢ Attention mechanism",
      "innovation3": "‚Ä¢ Scales to billions of",
      "innovation3b": "parameters"
    },
    "temperature": {
      "title": "Temperature: Creativity Knob",
      "lowLabel": "Low (0.0 - 0.3)",
      "lowDesc": "‚ùÑÔ∏è Predictable",
      "medLabel": "Medium (0.7)",
      "medDesc": "üéØ Balanced",
      "highLabel": "High (1.5 - 2.0)",
      "highDesc": "üî• Chaotic",
      "probDistribution": "Probability Distribution",
      "exampleOutput": "Example Output",
      "lowExample1": "\"The cat sat on the",
      "lowExample2": "mat. The cat sat on",
      "lowExample3": "the mat. The cat...\"",
      "lowNote": "(repetitive, safe)",
      "medExample1": "\"The cat rested on",
      "medExample2": "the warm mat,",
      "medExample3": "watching birds...\"",
      "medNote": "(natural, coherent)",
      "highExample1": "\"Quantum bicycle",
      "highExample2": "dreams! Purple",
      "highExample3": "elephant Tuesday...\"",
      "highNote": "(random, incoherent)"
    },
    "promptingBasics": {
      "title": "Asking Good Questions",
      "badPrompt": "‚ùå Bad Prompt",
      "badExample": "\"Tell me about food\"",
      "badResult1": "\"Food is a substance that...\"",
      "badResult2": "[vague essay about food]",
      "badResult3": "\"...provides nutrition...\"",
      "goodPrompt": "‚úÖ Good Prompt",
      "goodExample1": "\"List 3 vegetarian recipes",
      "goodExample2": "under 30 min with 20g+ protein\"",
      "goodResult1": "1. Chickpea curry (25 min, 22g)",
      "goodResult2": "2. Tofu stir-fry (20 min, 24g)",
      "goodResult3": "3. Lentil dal (28 min, 21g)",
      "principlesTitle": "5 Prompting Principles",
      "principle1": "Be specific",
      "principle1Desc": "State exactly what you want",
      "principle2": "Give context",
      "principle2Desc": "Provide relevant background",
      "principle3": "Set format",
      "principle3Desc": "Specify output structure",
      "principle4": "Use examples",
      "principle4Desc": "Show what you mean",
      "principle5": "Break complex tasks",
      "principle5Desc": "Split into smaller steps"
    },
    "contextEngineering": {
      "title": "Context Engineering ‚Äî Prompt Anatomy",
      "systemRole": "System Role",
      "systemRoleExample": "\"You are a helpful coding assistant...\"",
      "rules": "Rules & Constraints",
      "rulesExample": "\"Only use Python. Max 50 lines. No external libs.\"",
      "format": "Output Format",
      "formatExample": "\"Return JSON with keys: answer, confidence, source\"",
      "examples": "Examples (Few-Shot)",
      "examplesExample": "\"Input: 'hello' ‚Üí Output: {greeting: true}\"",
      "userQuery": "User Query",
      "conclusion": "Better context = Better AI output"
    },
    "ragPipeline": {
      "title": "RAG Pipeline ‚Äî Retrieval-Augmented Generation",
      "query": "Query",
      "queryExample": "\"What is...?\"",
      "retrieve": "RETRIEVE",
      "retrieveStep1": "1. Embed query",
      "retrieveStep2": "2. Vector search",
      "retrieveStep3": "3. Top-K results",
      "documentDb": "Document DB",
      "documentDbDesc": "Vectors + metadata",
      "augment": "AUGMENT",
      "augmentDesc1": "Combine query +",
      "augmentDesc2": "retrieved context",
      "generate": "GENERATE",
      "generateDesc1": "LLM produces",
      "generateDesc2": "grounded answer",
      "answer": "Answer",
      "answerLine1": "Grounded response with",
      "answerLine2": "sources cited from",
      "answerLine3": "retrieved documents"
    },
    "memoryTypes": {
      "title": "AI Memory Types",
      "shortTerm": "Short-Term Memory",
      "shortTermDesc": "(Context Window)",
      "message5": "üë§ Latest user message",
      "message4": "ü§ñ AI response",
      "message3": "üë§ Earlier message (fading...)",
      "shortTermProp1": "‚è±Ô∏è Limited to session",
      "shortTermProp2": "üìè Fixed token limit",
      "longTerm": "Long-Term Memory",
      "longTermDesc": "(External Storage)",
      "facts": "User facts",
      "preferences": "Preferences",
      "history": "Conversation history",
      "patterns": "Learned patterns",
      "longTermProp1": "üíæ Persists across sessions",
      "longTermProp2": "üîç Searchable via vectors",
      "store": "Store ‚Üí",
      "retrieveAction": "‚Üê Retrieve"
    },
    "lora": {
      "title": "LoRA ‚Äî Low-Rank Adaptation",
      "baseModel": "Base Model",
      "frozen": "üîí Frozen",
      "loraAdapters": "LoRA Adapters",
      "trained": "üî• Trained",
      "layer": "Layer",
      "frozenWeights": "Frozen weights",
      "whyLora": "Why LoRA?",
      "benefit1": "‚úì Train 0.1% of parameters",
      "benefit2": "‚úì 100x cheaper than full",
      "benefit3": "‚úì Swap adapters easily",
      "benefit4": "‚úì Keep base model intact",
      "input": "Input ‚Üì",
      "output": "Output ‚Üì"
    },
    "security": {
      "title": "AI Security ‚Äî Attack Surfaces",
      "inputZone": "INPUT ZONE",
      "modelZone": "MODEL ZONE",
      "outputZone": "OUTPUT ZONE",
      "threat": "‚ö†Ô∏è Threat",
      "defense": "üõ°Ô∏è Defense",
      "inputThreat": "Prompt injection attacks",
      "inputDefense": "Input validation & filtering",
      "modelThreat": "Training data poisoning",
      "modelDefense": "Alignment & RLHF",
      "outputThreat": "Harmful content generation",
      "outputDefense": "Output filtering & guardrails",
      "summaryTitle": "Defense in Depth: Protect at every layer",
      "summaryDesc": "No single defense is enough ‚Äî combine input, model, and output protections"
    },
    "agiAsi": {
      "title": "AI Capability Spectrum",
      "narrowAi": "Narrow AI",
      "today": "(Today)",
      "agi": "AGI",
      "future": "(Future?)",
      "agiDesc1": "Good at ALL",
      "agiDesc2": "human tasks",
      "asi": "ASI",
      "hypothetical": "(Hypothetical)",
      "asiDesc1": "Surpasses ALL",
      "asiDesc2": "human capability",
      "weAreHere": "We are here"
    },
    "functionCalling": {
      "title": "Function Calling: Giving AI Hands",
      "user": "USER",
      "aiModel": "AI MODEL",
      "function": "FUNCTION",
      "questionExample": "\"What's the weather?\"",
      "answerExample": "\"It's 5¬∞C and rainy\"",
      "aiGenerates": "AI generates the call",
      "appExecutes": "Your app executes it"
    },
    "greenAi": {
      "title": "Green AI: Energy Efficiency",
      "trainingCost": "Training Cost",
      "co2Emissions": "CO‚ÇÇ Emissions",
      "inferenceCost": "Inference Cost (per query)",
      "reduction": "90% reduction",
      "conclusion": "Same quality, fraction of the cost"
    },
    "moe": {
      "inputQuery": "Input Query",
      "router": "Router",
      "expert": "Expert",
      "combinedOutput": "Combined Output",
      "activeExperts": "Active: 2/8 experts",
      "fasterCheaper": "Faster + Cheaper"
    },
    "reasoningModels": {
      "title": "Regular LLM vs Reasoning Model",
      "regularLlm": "Regular LLM",
      "answer": "Answer: 425",
      "sometimesWrong": "(sometimes wrong)",
      "fast": "‚ö° Fast (instant)",
      "cheap": "üí∞ Cheap",
      "noVerification": "‚ö†Ô∏è No verification",
      "reasoningModel": "Reasoning Model",
      "verifiedAnswer": "Answer: 425 ‚úì",
      "slower": "üê¢ Slower (multi-step)",
      "verified": "‚úÖ Verified & accurate",
      "bestFor": "Best for: Math, Code, Logic & Complex Reasoning"
    }
  }
}
