{
  "metadata": {
    "title": "AI Knowledge Tree | AI Teadmiste Puu",
    "description": "Comprehensive interactive framework for teaching and understanding AI concepts. Terviklik interaktiivne raamistik AI kontseptide √µpetamiseks ja m√µistmiseks."
  },
  "header": {
    "title": "AI Knowledge Tree",
    "description": "Master AI fundamentals in ~2 hours",
    "treeView": "Concept Map",
    "treeViewAriaLabel": "Go to concept map view",
    "classicView": "Learning Path",
    "classicViewAriaLabel": "Back to learning path view"
  },
  "hero": {
    "title": "AI Knowledge Tree",
    "subtitle": "Understand AI well enough to make informed decisions. Master the fundamentals in ~2 hours.",
    "forWhom": "For trainers, team leads, and curious professionals",
    "classicViewTitle": "Learning Path",
    "classicViewDescription": "Step-by-step progression through concepts",
    "currentPage": "You are here",
    "treeViewTitle": "Concept Map",
    "treeViewDescription": "Visual overview of all concepts",
    "clickHere": "Click here ‚Üí",
    "or": "or",
    "treeEmoji": "Tree emoji",
    "bookEmoji": "Book",
    "startFromRoots": "Start from roots and move up or scroll freely",
    "newToAI": "New to AI?",
    "beginnerPathDesc": "Start here! Understand the basics in ~30 minutes.",
    "startWithTokens": "Start with Tokens",
    "startLearning": "Start Learning",
    "hook": "You already use AI. Now understand how it works.",
    "showMore": "Show more",
    "showLess": "Show less"
  },
  "brand": {
    "heroOrigin": "From dendron (Greek: tree) + dendrites (your neurons that grow when you learn)",
    "footerTitle": "Why Dendrix?",
    "footerStory": "Dendrites are tree-shaped brain cells that sprout new branches every time you learn something. Dendrix (dendron + dendrites) maps AI knowledge the way your brain actually stores it ‚Äî from roots to leaves."
  },
  "levels": {
    "roots": "Roots",
    "trunk": "Trunk",
    "branches": "Branches",
    "leaves": "Leaves",
    "totalTime": "Complete path: ~2 hours",
    "levelTime": {
      "roots": "~25 min",
      "trunk": "~30 min",
      "branches": "~35 min",
      "leaves": "~30 min"
    }
  },
  "treeView": {
    "title": "AI Knowledge Tree - Concept Map",
    "description": "All concepts in one view",
    "instructionsTitle": "Interactive Concept Map",
    "instructionsText": "Hover over a concept to highlight it. Click to open detailed view.",
    "ariaLabel": "Concept map",
    "diagramAriaLabel": "AI concepts map diagram",
    "legendHeading": "Tree levels legend",
    "rootsEmoji": "Seedling emoji",
    "trunkEmoji": "Evergreen tree emoji",
    "branchesEmoji": "Green plant emoji",
    "leavesEmoji": "Leaves emoji",
    "rootsLabel": "1. Roots",
    "trunkLabel": "2. Trunk",
    "branchesLabel": "3. Branches",
    "leavesLabel": "4. Leaves"
  },
  "viewMode": {
    "both": "Both",
    "simple": "Simple",
    "technical": "Technical",
    "ariaLabel": "Content mode selection",
    "tooltip": {
      "simple": "Simple metaphors and everyday examples",
      "technical": "Technical explanations for developers",
      "both": "Show both side by side"
    }
  },
  "nameToggle": {
    "simple": "Simple names",
    "technical": "Technical names",
    "ariaLabel": "Name display selection"
  },
  "darkMode": {
    "toggle": "Dark mode",
    "ariaLabel": "Dark/light mode selection",
    "switchToDark": "Switch to dark mode",
    "switchToLight": "Switch to light mode"
  },
  "settings": {
    "ariaLabel": "Open settings menu",
    "viewMode": "View Mode",
    "theme": "Theme",
    "language": "Language"
  },
  "welcome": {
    "title": "Welcome to AI Knowledge Tree",
    "subtitle": "Your guide to understanding AI ‚Äî in ~2 hours",
    "close": "Close",
    "skip": "Skip",
    "back": "Back",
    "next": "Next",
    "getStarted": "Get Started!",
    "stepOf": "Step {current} of {total}",
    "step1Title": "The Tree Metaphor",
    "step1Desc": "AI concepts organized like a growing tree",
    "step1Item1": "Roots ‚Äî Foundations: Tokens, Vectors, Attention (start here!)",
    "step1Item2": "Trunk ‚Äî Engineering: Prompting, Temperature, Context",
    "step1Item3": "Branches ‚Äî Applications: RAG, Agents, Function Calling",
    "step1Item4": "Leaves ‚Äî Trends: Green AI, Reasoning Models, MoE",
    "step2Title": "Two Ways to Explore",
    "step2Desc": "Choose the view that works for you",
    "step2Item1": "Learning Path ‚Äî Step-by-step progression from roots to leaves",
    "step2Item2": "Concept Map ‚Äî Visual overview of all concepts at once",
    "step2Item3": "Search (Cmd+K) ‚Äî Jump directly to any concept",
    "step3Title": "Track Your Progress",
    "step3Desc": "Learn at your own pace",
    "step3Item1": "Each concept takes 3-8 minutes to read",
    "step3Item2": "Mark concepts as learned to track progress",
    "step3Item3": "Switch between Simple and Technical explanations anytime"
  },
  "footer": {
    "description": "AI Knowledge Tree ‚Äì Interactive learning tool for understanding AI concepts",
    "version": "Version",
    "reportIssue": "Report Issue"
  },
  "complexity": {
    "level": "Complexity level",
    "beginner": "Beginner",
    "intermediate": "Intermediate",
    "advanced": "Advanced"
  },
  "loading": {
    "default": "Loading...",
    "concepts": "Loading concepts...",
    "tree": "Loading tree...",
    "content": "Loading content..."
  },
  "concept": {
    "simpleName": "Simple name",
    "metaphor": "Metaphor",
    "closeAriaLabel": "Close detailed view",
    "closeDialog": "Close dialog window",
    "viewDetails": "View details",
    "viewFullSize": "View full size",
    "beginnerPath": "Beginner Path",
    "readingTime": "min read",
    "learnFirst": "Learn first",
    "prerequisiteHelp": "Understanding these topics will help you better grasp this concept:",
    "navigateTo": "Navigate to topic",
    "simpleMetaphor": "Simple Metaphor",
    "technicalExplanation": "Technical Explanation",
    "codeExample": "Code Example",
    "pressEscToClose": "Press ESC or click outside to close",
    "backToTree": "Back to Tree",
    "share": "Share this concept",
    "shareNative": "Share",
    "shareTwitter": "Share on X (Twitter)",
    "shareLinkedIn": "Share on LinkedIn",
    "copyLink": "Copy link",
    "linkCopied": "Link copied!",
    "completed": "Completed",
    "markAsComplete": "Mark as understood",
    "markedComplete": "Understood!",
    "conceptMarkedComplete": "Concept marked as complete!",
    "undo": "Undo",
    "simpleMode": "Simple",
    "technicalMode": "Technical",
    "explanation": "Explanation",
    "visual": "Visual",
    "code": "Code",
    "whyThisCode": "Why this code?",
    "howToUse": "How to use",
    "tryItOut": "Try it out",
    "visualComingSoon": "Visual representation coming soon",
    "visualComingSoonDesc": "We're working on adding visual content for this concept.",
    "codeComingSoon": "Code example coming soon",
    "codeComingSoonDesc": "We're preparing a practical code example for this concept.",
    "swipeHint": "Swipe left/right for prev/next concept",
    "copyCode": "Copy code",
    "codeCopied": "Code copied!"
  },
  "tokenizer": {
    "title": "Tokenizer Demo",
    "subtitle": "See how AI sees text",
    "showInfo": "Show info",
    "hideInfo": "Hide info",
    "inputLabel": "Enter text to tokenize",
    "inputPlaceholder": "Write something to see how AI tokenizes text...",
    "tokensLabel": "Tokens:",
    "tokenCount": "{count} {count, plural, one {token} other {tokens}}",
    "tokensPlaceholder": "Tokens will appear here...",
    "infoTitle": "What are tokens?",
    "infoText": "Language models don't see text the way we do. They break text into smaller pieces called <em>tokens</em>. A token can be a word, part of a word, or even a single character. This demo shows a simplified version!",
    "estimatedCost": "Estimated cost (GPT-4)",
    "costTip": "<strong>Tip:</strong> Longer texts = more tokens = higher cost. Learn to optimize your prompts!",
    "tryExamples": "Try examples:",
    "example1": "Hello, world!",
    "example2": "GPT-4 costs 2024",
    "example3": "Artificial intelligence",
    "example4": "Machine learning is fascinating",
    "digitInsight": "<strong>Numbers split into digits!</strong> AI doesn't see \"2024\" as one number ‚Äî it becomes 4 separate tokens (2, 0, 2, 4). Each digit gets its own <em>vector</em> (numerical meaning). That's why AI can struggle with math ‚Äî it processes digits one by one, not as whole numbers.",
    "subwordInsight": "<strong>Long words get split!</strong> Tokens marked with <code>##</code> are subword pieces. AI breaks unfamiliar or long words into smaller known parts, similar to how you might sound out an unfamiliar word syllable by syllable.",
    "disclaimer": "This is a simplified tokenizer. Real language models use more sophisticated methods like BPE (Byte Pair Encoding) or WordPiece."
  },
  "navigation": {
    "viewOptions": "View options",
    "levelSections": "AI concept levels",
    "treeLevels": "Tree level navigation",
    "openNavPanel": "Open navigation panel",
    "closeNavPanel": "Close navigation panel",
    "goToLevel": "Go to {level} level",
    "currentLevel": "Current",
    "conceptsCompleted": "concepts understood",
    "backToTop": "Back to top",
    "switchToEstonian": "Switch to Estonian",
    "switchToEnglish": "Switch to English",
    "levels": {
      "roots": "Roots",
      "trunk": "Trunk",
      "branches": "Branches",
      "leaves": "Leaves"
    }
  },
  "journey": {
    "title": "Your Learning Path",
    "subtitle": "From everyday AI to deep understanding ‚Äî like growing a tree from roots to leaves.",
    "roots": "Foundation",
    "rootsDesc": "Tokens, vectors & the building blocks",
    "trunk": "Core Mechanics",
    "trunkDesc": "How models learn and generate",
    "branches": "Applications",
    "branchesDesc": "RAG, agents & real-world tools",
    "leaves": "Mastery",
    "leavesDesc": "Fine-tuning, safety & beyond"
  },
  "levelSection": {
    "conceptCount": "{count} concepts",
    "concepts": "{level} concepts"
  },
  "organicTree": {
    "clickForDetails": "Click for details",
    "simple": "Simple",
    "intermediate": "Intermediate",
    "advanced": "Advanced"
  },
  "search": {
    "button": "Search",
    "buttonLabel": "Open search (Cmd+K)",
    "placeholder": "Search concepts...",
    "inputLabel": "Search for AI concepts",
    "instructions": "Use arrow keys to navigate, Enter to select, and Escape to close",
    "close": "Close search",
    "noResults": "No concepts found",
    "tryDifferent": "Try a different search term",
    "recentSearches": "Recent Searches",
    "clearRecent": "Clear",
    "popularConcepts": "Popular Concepts",
    "navigate": "Navigate",
    "select": "Select",
    "resultsCount": "{count} {count, plural, one {result} other {results}}",
    "level": {
      "roots": "Roots",
      "trunk": "Trunk",
      "branches": "Branches",
      "leaves": "Leaves"
    }
  },
  "skillSelector": {
    "title": "Where should we start?",
    "subtitle": "Choose your experience level",
    "close": "Close modal",
    "beginner": {
      "title": "New to AI",
      "description": "Start from the basics. Learn what tokens, vectors, and transformers are.",
      "time": "~30 min to get started"
    },
    "intermediate": {
      "title": "Exploring AI",
      "description": "Know the basics? Dive into how models learn and generate content.",
      "time": "~45 min deeper dive"
    },
    "advanced": {
      "title": "Building with AI",
      "description": "Ready for advanced topics like fine-tuning, RAG, and agents.",
      "time": "~35 min advanced topics"
    },
    "skipAndExplore": "Skip and explore freely"
  },
  "vectorDemo": {
    "title": "Vector Similarity Demo",
    "subtitle": "Explore how AI understands word meanings",
    "showInfo": "Show info",
    "hideInfo": "Hide info",
    "reset": "Reset",
    "word": "Word",
    "optional": "(optional)",
    "wordPlaceholder": "e.g. cat, happy, king",
    "calculate": "Calculate Similarity",
    "similarityScores": "Similarity Scores:",
    "similarityHigh": "Very similar",
    "similarityMedium": "Somewhat similar",
    "similarityLow": "Not similar",
    "visualization": "2D Visualization:",
    "visualizationAriaLabel": "2D visualization of word similarities in vector space",
    "visualizationHint": "Words closer together are more similar in meaning. Hover over points for details.",
    "infoTitle": "What are embeddings?",
    "infoText": "AI represents words as vectors (lists of numbers) called <em>embeddings</em>. Similar words have similar vectors. This demo uses simplified 10-dimensional vectors, but real models like GPT use 1,536+ dimensions! The 2D plot shows these high-dimensional relationships projected into a space we can see.",
    "tryExamples": "Try examples:",
    "exampleLabel1": "king, queen, prince",
    "exampleWords1": "king, queen, prince",
    "exampleLabel2": "cat, dog, car",
    "exampleWords2": "cat, dog, car",
    "exampleLabel3": "happy, sad, angry",
    "exampleWords3": "happy, sad, angry",
    "exampleLabel4": "apple, banana, pizza",
    "exampleWords4": "apple, banana, pizza",
    "exampleLabel5": "computer, phone, tree",
    "exampleWords5": "computer, phone, tree",
    "disclaimer": "This demo uses pre-computed simplified embeddings. Real AI models create embeddings dynamically based on context and use thousands of dimensions for more nuanced understanding."
  },
  "codeBlock": {
    "copied": "Copied!",
    "copiedAriaLabel": "Copied!",
    "copy": "Copy",
    "copyAriaLabel": "Copy code",
    "explanation": "üí° Explanation: "
  },
  "quickJump": {
    "ariaLabel": "Scroll down and start learning",
    "startLearning": "Start learning"
  },
  "skeleton": {
    "loadingTree": "Loading tree..."
  },
  "toast": {
    "closeAriaLabel": "Close notification"
  },
  "visuals": {
    "attention": {
      "title": "Attention Mechanism ‚Äî Focus",
      "inputSentence": "Input sentence:",
      "word": {
        "went": "went",
        "store": "to store",
        "and": "and",
        "she": "she",
        "bought": "bought",
        "milk": "milk"
      },
      "processing": "When processing \"she\" (it/she/he):",
      "strong": "Strong (95%) ‚Äî refers to Mari",
      "medium": "Medium (72%)",
      "explanationTitle": "Attention shows which words are important for understanding each word",
      "explanationLine1": "\"she\" pays strong attention to \"Mari\" to know who is being referred to.",
      "explanationLine2": "Arrow thickness = attention strength. This helps the model understand relationships."
    },
    "prefillDecode": {
      "title": "Prefill vs Decode ‚Äî Reading vs Writing",
      "prefillTitle": "PREFILL ‚Äî Reading",
      "decodeTitle": "DECODE ‚Äî Writing",
      "token": {
        "write": "Write",
        "me": "me",
        "a": "a",
        "story": "story",
        "once": "Once",
        "upon": "upon"
      },
      "allAtOnce": "All processed at once",
      "fastParallel": "‚ö° Fast & Parallel",
      "prefillSpeed": "~50ms for 100 tokens",
      "oneAtATime": "One token at a time",
      "sequential": "üê¢ Sequential",
      "decodeSpeed": "~50ms per token",
      "decodeExample": "(100 tokens = 5 seconds)",
      "whyMatters": "Why This Matters",
      "explanationLine1": "Prefill processes your entire prompt instantly (parallel). Decode generates each new word one-by-one (sequential).",
      "explanationLine2": "This is why long responses take time, even though reading your question is instant."
    },
    "contextWindow": {
      "title": "Context Window ‚Äî Working Memory",
      "tokenFlow": "Token Flow:",
      "forgotten": "Forgotten",
      "pastLimit": "Past limit",
      "activeContext": "ACTIVE CONTEXT",
      "modelCanSee": "Model can \"see\" and use this",
      "available": "Available",
      "spaceLeft": "Space left",
      "flowDirection": "Tokens flow through window ‚Üí",
      "sizesByModel": "Context Window Sizes by Model",
      "gpt35Size": "4K tokens",
      "gpt4Size": "8K tokens",
      "gpt4TurboSize": "128K tokens",
      "claudeSize": "200K tokens",
      "visualScale": "Visual scale (1K = 1px width):",
      "explanation": "Think of it like RAM: larger context = can remember more of your conversation, but costs more and runs slower."
    },
    "hallucinations": {
      "title": "Hallucinations ‚Äî Confident Fabrications",
      "subtitle": "Both responses look equally confident, but one is completely wrong!",
      "hallucination": "‚ùå HALLUCINATION",
      "correct": "‚úì CORRECT",
      "question": "Q: Who built the Eiffel Tower?",
      "answerStart": "A: The Eiffel Tower was built in",
      "wrongFact": "1887 by Claude Monet",
      "correctFact": "1889 by Gustave Eiffel",
      "answerEnd": "as part of the World's Fair in Paris.",
      "confidence94": "Confidence: 94%",
      "confidence96": "Confidence: 96%",
      "problem": "‚ö†Ô∏è The Problem: Same confidence level!",
      "whyTitle": "Why Hallucinations Happen",
      "reason1": "‚Ä¢ LLMs generate text based on patterns, not facts from a database",
      "reason2": "‚Ä¢ They don't know when they're making things up ‚Äî they just predict likely next words",
      "reason3": "‚Ä¢ High confidence ‚â† accurate. The model can be very confident about wrong information",
      "tip": "‚úì Always verify important facts, especially dates, names, and technical details!"
    },
    "trainingInference": {
      "title": "Training vs Inference ‚Äî School vs Work",
      "trainingTitle": "TRAINING",
      "trainingPhase": "(Learning Phase)",
      "inferenceTitle": "INFERENCE",
      "inferencePhase": "(Using Phase)",
      "time": "Time",
      "cost": "Cost",
      "data": "Data",
      "modelWeights": "Model Weights",
      "trainingTime": "Weeks to Months",
      "trainingCost": "$2M - $100M+",
      "trainingGpus": "1,000s - 10,000s",
      "trainingData": "Terabytes",
      "weightsChanging": "CHANGING ‚ö°",
      "trainingDesc": "Learns patterns from billions of examples",
      "inferenceTime": "Milliseconds",
      "inferenceCost": "$0.002 per query",
      "inferenceGpus": "1 - 8",
      "inferenceData": "Your prompt",
      "weightsFrozen": "FROZEN üîí",
      "inferenceDesc": "Uses learned patterns to answer your questions",
      "explanation": "Training happens once (expensive). Inference happens millions of times per day (cheap & fast)."
    },
    "transformers": {
      "title": "Transformers ‚Äî Master Architecture",
      "subtitle": "The architecture behind GPT, Claude, Llama, and most modern AI models",
      "inputText": "INPUT TEXT",
      "tokenEmbedding": "Token Embedding",
      "blocksLabel": "TRANSFORMER BLOCKS √ó N (e.g., 12-96 layers)",
      "multiHeadAttention": "Multi-Head Attention üîç",
      "feedForward": "Feed-Forward + Layer Norm",
      "repeatedN": "(repeated N times)",
      "outputProbs": "Output Probabilities",
      "nextToken": "NEXT TOKEN",
      "annotationAttention1": "‚Üê Where words look",
      "annotationAttention2": "at each other",
      "annotationFF1": "‚Üê Processing &",
      "annotationFF2": "transformation",
      "annotationOutput1": "‚Üê Predicts next word",
      "annotationOutput2": "probabilities",
      "keyInnovation": "Key Innovation",
      "innovation1": "‚Ä¢ Parallel processing",
      "innovation2": "‚Ä¢ Attention mechanism",
      "innovation3": "‚Ä¢ Scales to billions of",
      "innovation3b": "parameters"
    },
    "temperature": {
      "title": "Temperature: Creativity Knob",
      "lowLabel": "Low (0.0 - 0.3)",
      "lowDesc": "‚ùÑÔ∏è Predictable",
      "medLabel": "Medium (0.7)",
      "medDesc": "üéØ Balanced",
      "highLabel": "High (1.5 - 2.0)",
      "highDesc": "üî• Chaotic",
      "probDistribution": "Probability Distribution",
      "exampleOutput": "Example Output",
      "lowExample1": "\"The cat sat on the",
      "lowExample2": "mat. The cat sat on",
      "lowExample3": "the mat. The cat...\"",
      "lowNote": "(repetitive, safe)",
      "medExample1": "\"The cat rested on",
      "medExample2": "the warm mat,",
      "medExample3": "watching birds...\"",
      "medNote": "(natural, coherent)",
      "highExample1": "\"Quantum bicycle",
      "highExample2": "dreams! Purple",
      "highExample3": "elephant Tuesday...\"",
      "highNote": "(random, incoherent)"
    },
    "promptingBasics": {
      "title": "Asking Good Questions",
      "badPrompt": "‚ùå Bad Prompt",
      "badExample": "\"Tell me about food\"",
      "badResult1": "\"Food is a substance that...\"",
      "badResult2": "[vague essay about food]",
      "badResult3": "\"...provides nutrition...\"",
      "goodPrompt": "‚úÖ Good Prompt",
      "goodExample1": "\"List 3 vegetarian recipes",
      "goodExample2": "under 30 min with 20g+ protein\"",
      "goodResult1": "1. Chickpea curry (25 min, 22g)",
      "goodResult2": "2. Tofu stir-fry (20 min, 24g)",
      "goodResult3": "3. Lentil dal (28 min, 21g)",
      "principlesTitle": "5 Prompting Principles",
      "principle1": "Be specific",
      "principle1Desc": "State exactly what you want",
      "principle2": "Give context",
      "principle2Desc": "Provide relevant background",
      "principle3": "Set format",
      "principle3Desc": "Specify output structure",
      "principle4": "Use examples",
      "principle4Desc": "Show what you mean",
      "principle5": "Break complex tasks",
      "principle5Desc": "Split into smaller steps"
    },
    "contextEngineering": {
      "title": "Context Engineering ‚Äî Prompt Anatomy",
      "systemRole": "System Role",
      "systemRoleExample": "\"You are a helpful coding assistant...\"",
      "rules": "Rules & Constraints",
      "rulesExample": "\"Only use Python. Max 50 lines. No external libs.\"",
      "format": "Output Format",
      "formatExample": "\"Return JSON with keys: answer, confidence, source\"",
      "examples": "Examples (Few-Shot)",
      "examplesExample": "\"Input: 'hello' ‚Üí Output: {greeting: true}\"",
      "userQuery": "User Query",
      "conclusion": "Better context = Better AI output"
    },
    "ragPipeline": {
      "title": "RAG Pipeline ‚Äî Retrieval-Augmented Generation",
      "query": "Query",
      "queryExample": "\"What is...?\"",
      "retrieve": "RETRIEVE",
      "retrieveStep1": "1. Embed query",
      "retrieveStep2": "2. Vector search",
      "retrieveStep3": "3. Top-K results",
      "documentDb": "Document DB",
      "documentDbDesc": "Vectors + metadata",
      "augment": "AUGMENT",
      "augmentDesc1": "Combine query +",
      "augmentDesc2": "retrieved context",
      "generate": "GENERATE",
      "generateDesc1": "LLM produces",
      "generateDesc2": "grounded answer",
      "answer": "Answer",
      "answerLine1": "Grounded response with",
      "answerLine2": "sources cited from",
      "answerLine3": "retrieved documents"
    },
    "memoryTypes": {
      "title": "AI Memory Types",
      "shortTerm": "Short-Term Memory",
      "shortTermDesc": "(Context Window)",
      "message5": "üë§ Latest user message",
      "message4": "ü§ñ AI response",
      "message3": "üë§ Earlier message (fading...)",
      "shortTermProp1": "‚è±Ô∏è Limited to session",
      "shortTermProp2": "üìè Fixed token limit",
      "longTerm": "Long-Term Memory",
      "longTermDesc": "(External Storage)",
      "facts": "User facts",
      "preferences": "Preferences",
      "history": "Conversation history",
      "patterns": "Learned patterns",
      "longTermProp1": "üíæ Persists across sessions",
      "longTermProp2": "üîç Searchable via vectors",
      "store": "Store ‚Üí",
      "retrieveAction": "‚Üê Retrieve"
    },
    "lora": {
      "title": "LoRA ‚Äî Low-Rank Adaptation",
      "baseModel": "Base Model",
      "frozen": "üîí Frozen",
      "loraAdapters": "LoRA Adapters",
      "trained": "üî• Trained",
      "layer": "Layer",
      "frozenWeights": "Frozen weights",
      "whyLora": "Why LoRA?",
      "benefit1": "‚úì Train 0.1% of parameters",
      "benefit2": "‚úì 100x cheaper than full",
      "benefit3": "‚úì Swap adapters easily",
      "benefit4": "‚úì Keep base model intact",
      "input": "Input ‚Üì",
      "output": "Output ‚Üì"
    },
    "security": {
      "title": "AI Security ‚Äî Attack Surfaces",
      "inputZone": "INPUT ZONE",
      "modelZone": "MODEL ZONE",
      "outputZone": "OUTPUT ZONE",
      "threat": "‚ö†Ô∏è Threat",
      "defense": "üõ°Ô∏è Defense",
      "inputThreat": "Prompt injection attacks",
      "inputDefense": "Input validation & filtering",
      "modelThreat": "Training data poisoning",
      "modelDefense": "Alignment & RLHF",
      "outputThreat": "Harmful content generation",
      "outputDefense": "Output filtering & guardrails",
      "summaryTitle": "Defense in Depth: Protect at every layer",
      "summaryDesc": "No single defense is enough ‚Äî combine input, model, and output protections"
    },
    "agentLoop": {
      "title": "AI Agent Loop",
      "observe": "OBSERVE",
      "think": "THINK",
      "act": "ACT",
      "toolSearch": "Search",
      "toolCode": "Code",
      "toolDatabase": "Database",
      "consultant": "Consultant",
      "consultantDesc": "Gives advice",
      "agent": "Agent",
      "agentDesc": "Takes action"
    },
    "mcpArchitecture": {
      "title": "MCP Architecture",
      "beforeMcp": "Before MCP",
      "customIntegration": "Custom integration",
      "forEachTool": "for each tool",
      "afterMcp": "After MCP",
      "mcpServer": "MCP Server",
      "analogy": "Like a universal adapter for AI tools"
    },
    "complexityLevels": {
      "title": "AI Complexity Levels",
      "llmChat": "LLM / Chat",
      "llmDesc": "Answers questions",
      "reasoningModel": "Reasoning Model",
      "reasoningDesc": "Thinks step by step",
      "agentDoer": "Agent",
      "agentDesc": "Takes action",
      "kitchenAnalogy": "Kitchen Analogy",
      "recipeBook": "Recipe Book",
      "providesInfo": "Provides info",
      "headChef": "Head Chef",
      "reasoning": "Reasoning",
      "plansMenu": "Plans the menu",
      "cook": "Cook",
      "agentLabel": "Agent",
      "makesFood": "Makes the food"
    },
    "agiAsi": {
      "title": "AI Capability Spectrum",
      "narrowAi": "Narrow AI",
      "today": "(Today)",
      "agi": "AGI",
      "future": "(Future?)",
      "agiDesc1": "Good at ALL",
      "agiDesc2": "human tasks",
      "asi": "ASI",
      "hypothetical": "(Hypothetical)",
      "asiDesc1": "Surpasses ALL",
      "asiDesc2": "human capability",
      "weAreHere": "We are here"
    },
    "functionCalling": {
      "title": "Function Calling: Giving AI Hands",
      "user": "USER",
      "aiModel": "AI MODEL",
      "function": "FUNCTION",
      "questionExample": "\"What's the weather?\"",
      "answerExample": "\"It's 5¬∞C and rainy\"",
      "aiGenerates": "AI generates the call",
      "appExecutes": "Your app executes it"
    },
    "greenAi": {
      "title": "Green AI: Energy Efficiency",
      "trainingCost": "Training Cost",
      "co2Emissions": "CO‚ÇÇ Emissions",
      "inferenceCost": "Inference Cost (per query)",
      "reduction": "90% reduction",
      "conclusion": "Same quality, fraction of the cost"
    },
    "moe": {
      "inputQuery": "Input Query",
      "router": "Router",
      "expert": "Expert",
      "combinedOutput": "Combined Output",
      "activeExperts": "Active: 2/8 experts",
      "fasterCheaper": "Faster + Cheaper"
    },
    "reasoningModels": {
      "title": "Regular LLM vs Reasoning Model",
      "regularLlm": "Regular LLM",
      "answer": "Answer: 425",
      "sometimesWrong": "(sometimes wrong)",
      "fast": "‚ö° Fast (instant)",
      "cheap": "üí∞ Cheap",
      "noVerification": "‚ö†Ô∏è No verification",
      "reasoningModel": "Reasoning Model",
      "verifiedAnswer": "Answer: 425 ‚úì",
      "slower": "üê¢ Slower (multi-step)",
      "verified": "‚úÖ Verified & accurate",
      "bestFor": "Best for: Math, Code, Logic & Complex Reasoning"
    }
  },
  "conceptData": {
    "moe": {
      "title": "Mixture of Experts (MoE)",
      "simpleName": "Council of Experts",
      "explanation": "MoE models activate only a few specialized 'experts' for each query instead of using the full model. Like a large company where different departments handle different tasks ‚Äî you don't need accounting to answer marketing questions. This makes the model faster and cheaper while maintaining quality.",
      "metaphor": "Imagine going to a large hospital with different specialists. The receptionist (router) reads your symptoms and sends you only to the relevant doctors ‚Äî not to all 100 specialists. If you have a skin issue, you see the dermatologist. If it's a broken bone, you see the orthopedist. The hospital is huge, but you only use 2-3 specialists per visit. That's exactly how MoE works: 8 expert sub-models, but only 2 activate per query."
    },
    "agi-asi": {
      "title": "AGI and ASI",
      "simpleName": "Superintelligence",
      "explanation": "AGI (Artificial General Intelligence) would be AI that matches human capability across all cognitive tasks ‚Äî learning, reasoning, creativity, common sense. ASI (Artificial Superintelligence) would surpass human intelligence in every domain. Today's AI is 'narrow' ‚Äî excellent at specific tasks but lacking general understanding. The path from narrow AI to AGI to ASI is uncertain, with estimates ranging from decades to never.",
      "metaphor": "Think of it as an educational journey: Today's AI is like a brilliant student who aces one exam but fails others (Narrow AI). AGI would be like Einstein ‚Äî world-class at everything from physics to music to conversation. ASI would be an intelligence so far beyond Einstein that we can't even comprehend what it would understand ‚Äî like trying to explain quantum physics to a goldfish."
    },
    "green-ai": {
      "title": "Green AI (Sustainability)",
      "simpleName": "Green AI",
      "explanation": "Training large AI models consumes enormous energy ‚Äî GPT-3 training emitted as much CO‚ÇÇ as 5 cars over their lifetime. Green AI focuses on making models more efficient: smaller models with similar performance, optimized training methods, better hardware utilization, and running inference efficiently. Techniques like model distillation, quantization, and efficient architectures reduce both cost and environmental impact.",
      "metaphor": "It's like the shift from gas-guzzling SUVs to electric cars. Early AI models were powerful but wasteful ‚Äî like a Hummer getting 10 mpg. Green AI is about getting the same performance from a Tesla ‚Äî optimized, efficient, and sustainable. You still get where you need to go, but with 90% less energy consumption."
    },
    "reasoning-models": {
      "title": "Specialized Reasoning Models",
      "simpleName": "The Thinker",
      "explanation": "Reasoning models (like OpenAI's o1 or o3) don't just predict the next word ‚Äî they 'think' through problems step-by-step before answering. They use chain-of-thought reasoning internally, checking their work and exploring different approaches. This makes them much better at complex tasks like mathematics, coding, and logic puzzles, though they're slower and more expensive than standard models.",
      "metaphor": "It's the difference between speed chess and tournament chess. A regular LLM is like a speed chess player ‚Äî fast, intuitive, often brilliant, but sometimes makes mistakes under time pressure. A reasoning model is like a grandmaster taking 30 minutes per move, considering multiple strategies, anticipating consequences, and double-checking before committing. The reasoning model is slower, but it rarely makes logical errors."
    },
    "ai-agents": {
      "title": "AI Agents",
      "simpleName": "The Doer",
      "explanation": "AI Agents are autonomous systems that can use tools, make decisions, and execute multi-step tasks with minimal human intervention. Unlike a chatbot that just responds, an agent can: read your emails, search the web, write code, run it, debug errors, and complete the task. They operate in a loop: observe ‚Üí reason ‚Üí act ‚Üí observe results ‚Üí continue. Agents use function calling and tool integration to interact with the real world.",
      "metaphor": "The difference between a consultant and a worker. A regular chatbot is a consultant: you ask a question, get advice, but YOU do the work. An AI agent is a worker you hire: you say 'organize my travel to New York next week,' and it books flights, finds hotels, adds calendar events, and sends you a summary. You supervise, but the agent handles execution."
    },
    "mcp": {
      "title": "MCP (Model Context Protocol)",
      "simpleName": "The Connector",
      "explanation": "MCP is an open standard that lets AI models securely connect to external data sources and tools ‚Äî databases, APIs, file systems, business software ‚Äî through a unified interface. Instead of each AI tool building custom integrations, MCP provides one protocol that works everywhere. It's like USB for AI: plug in once, work with any compatible model or tool.",
      "metaphor": "Remember when every phone had a different charging cable ‚Äî Nokia, iPhone, Samsung all needed different connectors? Then USB-C came along and solved everything. MCP is the same idea for AI. Before MCP, connecting Claude to your database required custom code; connecting GPT required different code. With MCP, you write one connector and any MCP-compatible AI can use it. One cable, all devices."
    },
    "complexity-levels": {
      "title": "3 Complexity Levels",
      "simpleName": "Three Levels",
      "explanation": "AI systems operate at three levels of sophistication: (1) LLM Level ‚Äî pure text prediction, fast, stateless; (2) Reasoning Level ‚Äî chain-of-thought, multi-step problem solving, slower but more accurate; (3) Agent Level ‚Äî autonomous task execution with tools, memory, and decision loops. Each level builds on the previous one, adding capability and complexity. You choose the level based on your task: simple questions ‚Üí LLM, complex reasoning ‚Üí Reasoning model, autonomous work ‚Üí Agent.",
      "metaphor": "Think of a kitchen: (1) Cookbook (LLM) ‚Äî you give it a recipe request, it spits out instructions. Fast, no thinking. (2) Chef (Reasoning) ‚Äî you ask for a meal, the chef plans the menu, considers ingredient availability, timing, and presentation. Takes longer but higher quality. (3) Personal Cook (Agent) ‚Äî you say 'I'm hungry,' and they check your fridge, order missing ingredients, cook the meal, plate it, and clean up. Full autonomy."
    },
    "function-calling": {
      "title": "Function Calling",
      "simpleName": "Giving AI Hands",
      "explanation": "Function calling lets LLMs call external functions to retrieve data or perform actions they can't do on their own. The model decides when to call a function, generates the parameters, your code executes it, and the model uses the result to continue the conversation. For example, 'What's the weather?' ‚Üí model calls get_weather(location='New York') ‚Üí your code fetches real data ‚Üí model formats the response. This turns LLMs from pure chatbots into systems that can interact with the real world.",
      "metaphor": "Imagine hiring a brilliant intern who knows everything but has no hands. They can analyze, write, plan ‚Äî but can't open doors, press buttons, or fetch files. Function calling is like giving them a phone to call specialists: 'Call the weather service for the forecast,' 'Call the database team for customer data,' 'Call the email API to send this message.' The intern (LLM) orchestrates; the specialists (functions) do the physical work."
    },
    "context-engineering": {
      "title": "Context Engineering",
      "simpleName": "Stage Direction",
      "explanation": "Context engineering means carefully constructing the full environment in which the AI operates ‚Äî not just the question, but the role, rules, format expectations, examples, and constraints. A well-engineered context includes: system role ('you are an expert Python developer'), constraints ('max 50 lines, no external libraries'), format ('return JSON'), and examples (few-shot learning). This dramatically improves response quality, consistency, and safety.",
      "metaphor": "Think of it like directing a theater actor. You don't just hand them a script and hope for the best. You set the stage (environment), define their character (role), explain the tone (constraints), show them examples of similar performances (few-shot), and specify the desired outcome (format). A vague prompt is like saying 'act.' A well-engineered context is a full director's brief that guides the performance exactly where you want it."
    },
    "rag": {
      "title": "RAG (Retrieval-Augmented Generation)",
      "simpleName": "The Library",
      "explanation": "RAG combines information retrieval with text generation. Instead of relying only on the model's training data (which can be outdated or incomplete), RAG first searches a database for relevant documents, then feeds them into the prompt as context. The flow: (1) User asks a question ‚Üí (2) System searches vector database for relevant docs ‚Üí (3) Top results are added to the prompt ‚Üí (4) LLM generates answer based on retrieved context. This grounds responses in your specific data and reduces hallucinations.",
      "metaphor": "It's like an open-book exam vs. a closed-book exam. Without RAG, the AI relies purely on memory ‚Äî what it learned during training. With RAG, the AI can quickly search through a library (your database), find relevant pages, and cite them while answering. You ask 'What's our refund policy?' ‚Üí RAG finds the official policy document ‚Üí AI quotes it directly. Always up-to-date, always grounded in real data."
    },
    "memory": {
      "title": "Memory and State Management",
      "simpleName": "Memory",
      "explanation": "AI memory comes in two forms: (1) Short-term ‚Äî the conversation context window (last N tokens the model can 'see'), temporary, limited by model capacity; (2) Long-term ‚Äî external storage (databases, vector stores) that persists across sessions. Short-term memory is like working memory; long-term is like keeping notes. Advanced systems combine both: storing facts, preferences, and conversation history in a database, then retrieving relevant pieces when needed.",
      "metaphor": "Think of short-term memory as your notebook during a meeting ‚Äî you can see the last few pages, but older notes scroll off. Long-term memory is your filing cabinet ‚Äî everything is saved permanently, and you can search for it later. A smart assistant might remember 'user prefers dark roast coffee' in long-term storage, then retrieve it weeks later when you ask about coffee recommendations."
    },
    "lora": {
      "title": "LoRA & Fine-Tuning",
      "simpleName": "Continuing Education",
      "explanation": "LoRA (Low-Rank Adaptation) is a technique for fine-tuning large models efficiently. Instead of retraining all billions of parameters (expensive, slow), LoRA freezes the base model and trains small 'adapter' layers ‚Äî just 0.1% of the parameters. This makes fine-tuning 100x cheaper and faster while achieving similar results. You can swap adapters easily: one for medical text, one for legal, one for code. The base model stays unchanged.",
      "metaphor": "Imagine you have a brilliant university graduate (base model) who knows general knowledge. Instead of sending them back to university for 4 years (full fine-tuning) to specialize in law, you give them a short 3-month course (LoRA adapter). They keep all their general knowledge but gain specialized legal expertise. Later, you can swap the legal course for a medical course, and they adapt instantly. Same graduate, different specializations, minimal retraining."
    },
    "security": {
      "title": "AI Security",
      "simpleName": "Security",
      "explanation": "AI security involves protecting against three main attack surfaces: (1) Input attacks ‚Äî prompt injection, jailbreaking attempts to bypass rules; (2) Model attacks ‚Äî training data poisoning, extracting private information the model memorized; (3) Output attacks ‚Äî generating harmful content, leaking sensitive data. Defenses include input validation, output filtering, alignment training (RLHF), and defense-in-depth strategies that protect at every layer.",
      "metaphor": "Think of AI security like a nightclub bouncer working at three checkpoints: (1) The entrance (input) ‚Äî checking IDs, rejecting troublemakers trying to sneak in with fake prompts; (2) The interior (model) ‚Äî trained staff ensuring guests behave, no one's spiking drinks (poisoning training data); (3) The exit (output) ‚Äî making sure no one leaves with stolen property or dangerous items (harmful outputs). You need protection at all three points, not just one."
    },
    "temperature-sampling": {
      "title": "Temperature & Sampling",
      "simpleName": "Creativity Knob",
      "explanation": "Temperature controls the randomness of AI outputs by adjusting the probability distribution over next-token predictions. Low temperature (0.0-0.3) makes the model deterministic and conservative ‚Äî always picking the most likely word. High temperature (1.0-2.0) flattens the distribution, making unlikely words more probable, increasing creativity but risking incoherence. For factual tasks, use low temperature. For creative writing or brainstorming, use higher values. Sampling strategies like top-k and top-p further refine this control.",
      "metaphor": "Temperature is like the dial on a creativity machine. At 0 (freezing cold), the machine is a calculator ‚Äî always gives the same answer, predictable, safe. At 0.7 (room temperature), it's balanced ‚Äî natural, varied, but still coherent. At 2.0 (boiling hot), it's a wild artist throwing paint randomly ‚Äî creative chaos, often nonsensical. For 'What is 2+2?' use cold (0). For 'Write a surrealist poem,' use hot (1.5)."
    },
    "prompting-basics": {
      "title": "Prompting Basics",
      "simpleName": "Asking Good Questions",
      "explanation": "Effective prompting follows key principles: (1) Be specific ‚Äî vague requests get vague answers; (2) Provide context ‚Äî background info helps the model understand your needs; (3) Specify format ‚Äî tell the model how to structure the output (bullet points, JSON, table); (4) Use examples ‚Äî show the model what you want (few-shot learning); (5) Break down complex tasks ‚Äî chain multiple simpler prompts instead of one giant request. Good prompts are clear, structured, and provide enough information for the model to succeed.",
      "metaphor": "It's like asking someone for dinner recommendations. Bad: 'Tell me about food.' (Too vague! Which food? For what occasion?) Good: 'Suggest 3 vegetarian restaurants in downtown Seattle, under $30 per person, open on Sundays, with outdoor seating.' The second prompt is specific (vegetarian), gives context (location, budget, day), sets format (3 suggestions), and defines constraints (outdoor seating). You'll get exactly what you need."
    },
    "training-vs-inference": {
      "title": "Training vs Inference",
      "simpleName": "School vs Work",
      "explanation": "Training is the expensive, one-time process of teaching the model by exposing it to billions of text examples, adjusting trillions of parameters over weeks or months. This costs millions of dollars and requires thousands of GPUs. Inference is the cheap, fast process of using the trained model to answer questions ‚Äî milliseconds per query, costing fractions of a cent. Training happens once; inference happens millions of times daily. The model weights are frozen during inference ‚Äî no learning happens when you chat with ChatGPT.",
      "metaphor": "Training is like going to medical school ‚Äî years of study, huge investment, learning from thousands of cases. Inference is like a doctor seeing patients ‚Äî quick consultations using knowledge already learned. The doctor doesn't relearn medicine for each patient; they apply existing expertise. Similarly, ChatGPT doesn't retrain when you ask a question; it applies patterns learned during training. One expensive education, millions of cheap applications."
    },
    "tokens": {
      "title": "Tokens",
      "simpleName": "Text Blocks",
      "explanation": "Tokens are the smallest units of text that AI models process. A token can be a word ('hello'), part of a word ('auto' + 'mobile'), or even a single character. English words average ~1.3 tokens; uncommon words may split into multiple tokens. Tokens matter because they determine: (1) Cost ‚Äî APIs charge per token; (2) Speed ‚Äî more tokens = slower processing; (3) Context limits ‚Äî models have maximum token capacity (e.g., 128K tokens = ~96K words). Understanding tokenization helps you optimize prompts and predict costs.",
      "metaphor": "Think of tokens as Lego blocks used to build sentences. Common words like 'the' or 'is' are single blocks. Longer or rare words like 'antidisestablishmentarianism' might be broken into several smaller blocks that snap together. The AI doesn't see letters; it sees these blocks. When you pay for API usage, you're paying per block, not per word. So efficient prompting means using fewer blocks to say the same thing."
    },
    "vectors": {
      "title": "Vectors (Embeddings)",
      "simpleName": "Meaning Map",
      "explanation": "Vectors (embeddings) represent words and concepts as lists of numbers in high-dimensional space. Similar meanings cluster together ‚Äî 'king' and 'queen' are close; 'king' and 'banana' are far apart. This numerical representation enables AI to perform semantic math: king - man + woman ‚âà queen. Modern models use 1,536+ dimensions. Vectors power semantic search, recommendations, and similarity matching. They're generated by neural networks trained to place related concepts near each other in this abstract space.",
      "metaphor": "Imagine every word has GPS coordinates, but in 1,536-dimensional space instead of just latitude/longitude. Words with similar meanings are geographically close. 'Happy' and 'joyful' might be neighbors at coordinates [0.2, 0.8, 0.3...]. 'Sad' is far away at [-0.5, -0.2, 0.1...]. When you search 'cheap flights,' the system finds documents near those coordinates in meaning-space, even if they use different words like 'affordable airfare.' Distance in this space = similarity in meaning."
    },
    "attention": {
      "title": "Attention Mechanism",
      "simpleName": "Focus",
      "explanation": "Attention lets models dynamically focus on relevant parts of the input when processing each word. When reading 'The cat sat on the mat,' the model can look back at 'cat' when processing 'it' later. Attention computes a weighted score for every word pair ‚Äî high scores mean strong relationship, low scores mean weak. Multi-head attention uses several parallel attention mechanisms to capture different types of relationships (syntax, semantics, etc.). This mechanism replaced older sequential models and enabled the transformer revolution.",
      "metaphor": "Imagine reading a sentence with a flashlight in a dark room. When you reach the word 'she,' you shine the light back to find who 'she' refers to ‚Äî maybe 'Maria' mentioned earlier. The brightness of the light (attention weight) is strongest on 'Maria' and dimmer on irrelevant words like 'the' or 'and.' The model does this for every word, simultaneously, building a web of relationships. That's attention: dynamically highlighting what matters for understanding each piece of text."
    },
    "prefill-decode": {
      "title": "Prefill vs Decode",
      "simpleName": "Reading vs Writing",
      "explanation": "LLM inference has two phases: (1) Prefill ‚Äî processing your entire input prompt in parallel, building the context representation. Fast, parallel computation. (2) Decode ‚Äî generating the response one token at a time, sequentially. Slow, each token depends on the previous ones. Prefill might take 50ms for a 1000-token prompt; decoding might take 50ms per token. This is why reading your question is instant but generating a long answer takes time. Optimizations like KV-caching speed up decoding by reusing computations.",
      "metaphor": "Think of prefill as reading a whole book page at once ‚Äî your eyes scan the entire page in seconds (parallel processing). Decoding is like handwriting a response, one letter at a time ‚Äî you can't write the 10th letter before the 9th (sequential). This is why ChatGPT 'understands' your long question instantly (prefill) but streams the answer slowly, word by word (decode). Reading is fast; writing takes time."
    },
    "context-windows": {
      "title": "Context Windows",
      "simpleName": "Working Memory",
      "explanation": "The context window is the maximum amount of text (in tokens) a model can process at once ‚Äî both your input and its output combined. GPT-3.5 has 4K tokens (~3K words); GPT-4 has 8K-128K; Claude has up to 200K. When you exceed the limit, older tokens get 'forgotten' ‚Äî they scroll out of the window. Larger windows enable longer conversations, bigger documents, and more context, but cost more and run slower. Managing context effectively is crucial for complex tasks.",
      "metaphor": "Think of the context window as RAM (working memory) in a computer. A model with a 4K token window is like having 4GB RAM ‚Äî fine for simple tasks, but you can't load huge files. A 200K window is like 64GB RAM ‚Äî you can work with entire books at once. When you exceed capacity, the system 'forgets' old data to make room for new. A chatbot with a small window might forget what you said 10 messages ago, just like you might forget details from a book's first chapter by the time you reach the end."
    },
    "hallucinations": {
      "title": "Hallucinations",
      "simpleName": "Confident Fabrications",
      "explanation": "Hallucinations occur when AI generates plausible-sounding but factually incorrect information. LLMs predict the next word based on patterns, not truth ‚Äî they don't access a fact database. If the model hasn't seen correct info or the question is ambiguous, it may 'fill in the blanks' with convincing nonsense. The danger: hallucinations are presented with the same confidence as facts. Mitigation strategies include RAG (grounding in real data), citations, fact-checking, and using smaller temperatures for factual tasks.",
      "metaphor": "Imagine asking a student a question they don't know the answer to. Instead of saying 'I don't know,' they confidently make up an answer that sounds right. 'Who invented the telephone?' ‚Äî 'Thomas Edison, in 1875, in Boston.' Sounds plausible! But it's wrong (Alexander Graham Bell, 1876). The student isn't lying; they're pattern-matching from partial knowledge. AI does the same: it generates statistically likely text, not verified truth. Always fact-check important claims, just like you would with a confident but unreliable student."
    },
    "transformers": {
      "title": "Transformers",
      "simpleName": "Master Architecture",
      "explanation": "Transformers are the neural network architecture that powers nearly all modern LLMs (GPT, Claude, Llama, etc.). Introduced in 2017, transformers revolutionized AI by enabling parallel processing and long-range dependencies through attention mechanisms. The architecture has two main components: multi-head attention (which words relate to which) and feed-forward layers (transformation and processing). Stacking dozens of these layers creates models with billions of parameters. Transformers replaced older sequential models (RNNs) and enabled the current AI boom.",
      "metaphor": "Think of transformers as the architectural innovation that enabled skyscrapers. Before steel-frame construction, buildings were limited to ~10 stories due to structural constraints. Steel frames changed everything ‚Äî suddenly you could build 100+ stories by stacking modular, reinforced units. Transformers did the same for AI: before them, models were limited by sequential processing (like brick-stacking). Transformers introduced parallel processing and attention (like steel frames), enabling massive scale. Now we stack 96 transformer layers to build GPT-4, just like stacking floors to build the Empire State Building."
    }
  },
  "conceptLevels": {
    "roots": {
      "name": "ROOTS",
      "subtitle": "Fundamental Mechanics",
      "description": "This is the AI 'engine'. Without understanding how the machine processes language, you're operating blind."
    },
    "trunk": {
      "name": "TRUNK",
      "subtitle": "Engineering & Architecture",
      "description": "The trunk is the supporting structure that holds everything else up."
    },
    "branches": {
      "name": "BRANCHES",
      "subtitle": "Applications & Agents",
      "description": "Branches are the practical application of knowledge."
    },
    "leaves": {
      "name": "LEAVES & FRUITS",
      "subtitle": "Research & Trends",
      "description": "This is the fastest-changing part of the tree. What is cutting-edge today is standard tomorrow."
    }
  },
  "codeSnippets": {
    "tokens": {
      "sampleText": "Hello, how are you?",
      "comment_load": "Load tokenizer for GPT-4",
      "comment_tokenize": "Tokenize text",
      "comment_decode": "Decode back to text"
    },
    "vectors": {
      "word1": "king",
      "word2": "queen",
      "word3": "banana",
      "comment_create": "Create embeddings",
      "comment_similarity": "Calculate similarity (cosine similarity)",
      "result1": "King vs Queen",
      "result2": "King vs Banana"
    },
    "attention": {
      "comment_simplified": "Simplified attention calculation",
      "comment_current": "Current word",
      "comment_previous": "Previous words",
      "comment_simplified2": "Simplified",
      "comment_scores": "Calculate attention scores",
      "word1": "Cat",
      "word2": "sat",
      "word3": "chair",
      "comment_apply": "Apply attention"
    },
    "ai-agents": {
      "comment_tools": "Define available tools",
      "comment_loop": "Agent loop",
      "userMessage": "What is the weather in London?",
      "toolResult_temp": "5¬∞C",
      "toolResult_cond": "cloudy",
      "comment_execute": "Execute tool",
      "comment_final": "Final answer"
    },
    "context-engineering": {
      "comment_poor": "Poor context - vague prompt",
      "poorPrompt": "Write an email",
      "systemPrompt": "You are a professional business communication specialist.\n      \n      Rules:\n      - Use a formal tone\n      - Keep emails short (max 150 words)\n      - Always include a CTA (call-to-action)\n      - Avoid jargon\n      \n      Format:\n      Greeting -> Context -> Main message -> CTA -> Closing",
      "userPrompt": "Write an email to a client asking about product delivery time"
    },
    "rag": {
      "comment_kb": "Knowledge base",
      "doc1": "The capital of Estonia is Tallinn.",
      "doc2": "Tallinn is located on the Gulf of Finland.",
      "doc3": "Python is a popular programming language.",
      "comment_query": "User query",
      "query": "What is the capital of Estonia?",
      "comment_retrieve": "1. Retrieve: Find relevant documents",
      "comment_augment": "2. Augment: Add context to prompt",
      "promptTemplate": "Context: {top_doc}\\n\\nQuestion: {query}",
      "comment_generate": "3. Generate: Get answer"
    },
    "function-calling": {
      "comment_define": "Define available functions",
      "toolDescription": "Get weather forecast for a city",
      "paramDescription": "City name",
      "comment_user": "User asks about weather",
      "userMessage": "What will the weather be like tomorrow in London?",
      "comment_decides": "Model decides to call function",
      "comment_execute": "Execute function (simulated)",
      "toolResult_temp": "8¬∞C",
      "toolResult_cond": "rainy",
      "comment_sendBack": "Send result back to model",
      "comment_getFinal": "Get final response"
    }
  },
  "codeExplanations": {
    "tokens": "This example shows how the tiktoken library converts text into tokens. Estonian words may require more tokens than English words.",
    "vectors": "Vectors convert words into numbers. Words with similar meanings (king/queen) are closer in space than different words (king/banana).",
    "attention": "The attention mechanism calculates the importance of each word relative to other words. Higher scores indicate stronger relationships.",
    "context-engineering": "Context engineering means creating a systematic environment: define the role, rules, format, and goal. This makes AI responses much more accurate and useful.",
    "rag": "RAG finds relevant documents (Retrieve), adds them to the prompt (Augment), and generates an answer (Generate). This allows AI to respond accurately based on company-specific data.",
    "ai-agents": "AI Agents use a loop: think ‚Üí choose tool ‚Üí execute ‚Üí check result ‚Üí repeat if needed. This enables autonomous task completion.",
    "function-calling": "Function calling allows the AI to 'decide' when to use external tools. The model generates a structured call, your application executes it, and the model uses the result to compose the final answer."
  },
  "learningPaths": {
    "title": "Learning Paths",
    "subtitle": "Curated journeys through AI concepts. Choose a path that matches your goals.",
    "backToHome": "Back to Home",
    "backToPaths": "All Learning Paths",
    "concepts": "concepts",
    "minutes": "min",
    "difficulty": "Difficulty",
    "beginner": "Beginner",
    "intermediate": "Intermediate",
    "advanced": "Advanced",
    "estimatedTime": "Estimated time",
    "prerequisites": "Prerequisites",
    "noPrerequisites": "No prerequisites",
    "startPath": "Start Learning",
    "continuePath": "Continue Learning",
    "completed": "Completed",
    "progress": "Progress",
    "conceptsCompleted": "{count} of {total} completed",
    "nextConcept": "Next up",
    "pathComplete": "Path complete!",
    "pathCompleteDesc": "You've completed all concepts in this learning path.",
    "exploreMore": "Explore more paths",
    "openConcept": "Open concept",
    "ai-fundamentals": {
      "title": "AI Fundamentals",
      "description": "Build a solid foundation. Understand tokens, vectors, attention, and how transformers work under the hood.",
      "longDescription": "This path takes you from zero to fluent in the core mechanics of modern AI. You'll learn how text becomes numbers, how models find meaning, and why transformers changed everything."
    },
    "prompt-engineering": {
      "title": "Prompt Engineering",
      "description": "Master the art of communicating with AI. Learn context windows, temperature control, and effective prompting strategies.",
      "longDescription": "Effective prompting is the most practical AI skill you can learn today. This path teaches you how models read your input, how to control creativity vs. precision, and how to structure prompts for consistent results."
    },
    "build-rag-apps": {
      "title": "Build RAG Applications",
      "description": "Learn to build AI apps that use your own data. From embeddings to retrieval-augmented generation.",
      "longDescription": "RAG is the most popular pattern for building enterprise AI applications. This path covers vector embeddings, context management, retrieval pipelines, and memory ‚Äî everything you need to build AI that knows your data."
    },
    "ai-agents-path": {
      "title": "AI Agents & Tools",
      "description": "Build autonomous AI systems that reason, use tools, and interact with external services.",
      "longDescription": "AI agents represent the next frontier. This path covers context engineering, function calling, agent loops, the Model Context Protocol (MCP), memory management, and security ‚Äî everything needed to build reliable autonomous AI systems."
    },
    "fine-tuning-path": {
      "title": "Fine-Tuning & Model Training",
      "description": "Understand model training, inference optimization, and techniques like LoRA and Mixture of Experts.",
      "longDescription": "Go deeper into how models are built and customized. This path covers the training pipeline, efficient fine-tuning with LoRA, inference optimization, and advanced architectures like Mixture of Experts."
    }
  },
  "upNext": {
    "title": "Up Next",
    "sameLevel": "Continue in this level",
    "nextLevel": "Next level",
    "prerequisiteFor": "Unlocks",
    "allDone": "You've explored all concepts!",
    "allDoneDesc": "Great job! Browse the tree to revisit any concept.",
    "browseTree": "Browse Tree"
  },
  "celebration": {
    "levelComplete": "Level Complete!",
    "levelCompleteDesc": "You've completed all {count} concepts in {level}.",
    "allComplete": "Tree Complete!",
    "allCompleteDesc": "You've explored all {count} concepts. You've grown from roots to leaves!",
    "nextLevel": "Continue to {level}",
    "shareProgress": "Share Progress",
    "keepExploring": "Keep Exploring",
    "awesome": "Awesome!",
    "certificate": "Certificate of Completion"
  }
}
